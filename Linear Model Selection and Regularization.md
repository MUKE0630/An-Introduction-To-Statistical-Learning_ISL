# Linear Model Selection and Regularization

标准线性模型：

$$
Y=\beta_0+\beta_1X_1+\cdots+\beta_pX_p+\epsilon\tag{6.1}
$$

我们一般使用最小二乘法来拟合模型，但这一章我们使用另一个拟合过程代替最小二乘，以此来产生更好的预测精度和模型可解释性。

- 预测精度：如果响应变量和预测变量之间的真实关系是近似线性的，那么最小二乘估计将具有较低的偏差。如果观测值 $n$ >> 变量 $p$ ，最小二乘也有较低的方差。然而，如果 $n$ 不比 $p$ 大很多，那么最小二乘拟合可能存在很大的变异性，导致过拟合。如果 $p>n$ 则不再有唯一的最小二乘估计，此时方差位无穷大，该方法无法使用。通过约束或缩小估计系数，我们往往可以以可忽略的偏差增加为代价大幅降低方差。这可以导致准确性有实质性的提高，我们可以预测未用于模型训练的观测值的响应。
- 模型可解释性：通常情况下，在多元回归模型中使用的一些或许多变量实际上与响应不相关。包含这些不相关的变量导致了结果模型中不必要的复杂性。通过移除这些变量|——即通过将相应的系数估计值置零——我们可以得到一个更容易解释的模型。现在最小二乘极不可能得到任何精确为零的系数估计值。在本章中，我们看到了一些自动执行特征选择或变量选择的方法——即从多元回归模型中排除无关变量

现在由许多经典或现代的方法使最小二乘拟合，这一章讨论三种重要方法

- 子集选择：这种方法涉及识别我们认为与响应变量相关的  $p$ 个预测因子的子集。然后我们在减少后的变量集上使用最小二乘法建立模型。
- 收缩：该方法涉及一个包含所有p个预测变量的模型。然而，估计系数相对于最小二乘估计值向零收缩。这种收缩(又称为正则化)具有减小方差的作用。根据执行何种类型的收缩，一些系数可能被估计为恰好为零。因此，收缩方法也可以进行变量选择。
- 降维：该方法将p个预测变量投影到一个M维子空间，其中M < p。这是通过计算变量的M种不同的线性组合或投影来实现的。然后将这些M个投影作为预测因子，通过最小二乘法建立线性回归模型。

## 子集选择（****************subset selection****************）

在本节中我们考虑了一些选择预测变量子集的方法。

### 最佳子集选择

> 为了进行最佳子集选择，我们对 $p$ 个预测变量的每个可能组合进行单独的最小二乘回归
> 

但这会产生 $2^p$ 个模型，可将其优化为以下算法：

1. 令 $M_0$ 代表不包含预测变量的空模型（null model）。该模型简单地预测每个观测的样本均值。
2. 对于 $k=1,2,\dots,p$
    1. 拟合所有 $\binom{p}{k}$ 包含 $k$  个预测因子的模型
    2. 在 $\binom{p}{k}$ 之中选择最好的模型，$M_k$ ，即有最低 $RSS$ 或最大 $R^2$
3. 在 $M_0,\dots,M_p$ 中选择单个最好模型，使用交叉验证的预测误差

步骤 2 确定每个子集大小上的最佳模型(在训练数据上)，以将问题从 $2^p$ 个可能模型中的一个减少到 $p + 1$ 个可能模型中的一个。

从这 $p+1$ 个模型中选择最好模型需要小心，因为随着包含的特征数量增多，这些模型的 $RSS$ 单调降低， $R^2$ 单调增加。因此，如图6.1，如果我们使用这些统计量来选择最佳模型，那么我们总会得到一个包含所有变量的模型。

![图 6.1](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled.png)

图 6.1

问题在于，低 $RSS$ 或高 $R^2$ 表示训练误差低的模型，而我们希望选择测试误差低的模型。所以第三步采用交叉验证的预测错误来选择模型。

最佳子集简单，但受到计算限制，当 $p$ 增大时，需要拟合的模型与计算量呈指数增加。接下来介绍一些计算高效的最佳子集选择方案。

### 逐级选择

- 前向逐步选择
    
    > 前向逐步选择从不包含预测变量的模型开始，然后向模型添加预测变量，一次一个，直到所有预测变量都在模型中。 特别是，在每个步骤中，将对拟合提供最大额外改进的变量添加到模型中。
    > 
    
    具体算法如下：
    
    1. 令 $M_0$ 代表空模型，不包含预测因子
    2. 对于 $k =0,\dots,p-1$
        1. 考虑所有  $p-k$ 模型，这些模型用一个额外的预测变量来增加 $M_k$ 中的预测变量。
        2. 在这些 $p - k$ 个模型中选择最优的，称之为 $M_{k + 1}$ 。这里的 ”最优“ 定义为 $RSS$ 最小或 $R^2$ 最大。
    3. 在 $M_0,\dots,M_p$ 中选择单个最好模型，使用交叉验证的预测误差
    
    在算法的步骤 2(b) 中，我们必须从那些用一个额外的预测器增加 $M_k$ 的 $p - k$ 中找出最好的模型。我们可以通过简单地选择 $RSS$ 最低或 $R^2$ 最高的模型来实现
    
    前向逐级选择在计算性上是明显优于最佳子集选择的。尽管该方法在实际中表现良好，但在包含 $p$ 个预报因子的所有 $2^p$ 个模型中，并不能保证找到最佳的可能模型。
    
    例如，假设在给定的 $p = 3$ 个预测变量的数据集中，最好可能的单变量模型包含 $X_1$，最好可能的双变量模型则包含 $X_2$ 和 $X_3$ 。然后向前逐步选择将无法选择最佳可能的两变量模型，因为 $M_1$ 将包含$X_1$，所以$M_2$ 也必须包含 $X_1$ 和一个额外的变量
    
- 后向逐级选择
    
    > 它从包含所有 $p$ 个预报因子的全最小二乘模型开始，然后迭代地移除最小有用的预报因子，一次一个
    > 
    
    算法描述如下：
    
    1. 令 $M_0$ 代表完全模型，包含所有的 $p$  预测变量
    2. 对于 $k=p,p-1,\dots,1$
        1. 考虑 $M_k$ 中包含除一个预报因子外的所有 $k$ 个模型，共有 $k - 1$ 个预报因子
        2. 在这 $k$ 个模型中选择最优的，称之为 $M_{k - 1}$ 。这里的 ”最优“ 定义为 $RSS$ 最小或 $R^2$ 最大。
    3. 在 $M_0,\dots,M_p$ 中选择单个最好模型，使用交叉验证的预测误差
    
    与前向逐步选择一样，后向选择方法只通过 $1 + p( p + 1) /2$ 个模型进行搜索。与前向逐步选择一样，后向逐步选择也不能保证得到最佳模型。
    
    后向选择要求样本个数 $n$ 大于变量个数 $p$  (使得整个模型可以拟合)。相比之下，前向逐级即使在 $n < p$ 时也可以使用，也是当 $p$ 很大时唯一可行的子集法
    

- 混合方法
    
    最佳子集、向前逐步和向后逐步选择方法通常会给出相似但不相同的模型。 作为另一种选择，可以使用向前和向后逐步选择的混合版本，其中变量按顺序添加到模型中，类似于向前选择。 然而，在添加每个新变量之后，该方法还可以删除不再提供模型拟合改进的任何变量。 这种方法试图更接近地模拟最佳子集选择，同时保留向前和向后逐步选择的计算优势。
    

### 选择最优模型

因此，RSS和R2不适合在预测变量个数不同的模型集合中选择最佳模型。为了选择关于测试误差的最佳模型，我们需要估计这个测试误差。常见的方法有两种：

- 我们可以通过调整训练误差来间接估计试验误差，以考虑过度拟合带来的偏差。
- 我们可以直接估计测试误差，使用验证集方法或交叉验证方法

下面我们考察这两种方法

- $C_p,AIC,BIC,$ 调整后的 $R^2$
    - $C_p$ 估计测试 $MSE$ 的计算方程
        
        $$
        C_p=\dfrac{1}{n}\left(\mathrm{RSS}+2d\hat{\sigma}^2\right)\tag{6.2}
        $$
        
        其中 $d$ 是预测因子数， $$$\sigma^ 2$ 是式( 6.1 )中与每个响应测量相关的误差的方差的估计。通常使用包含所有预测变量的全模型来估计$\sigma^ 2$。
        
        本质上，$C_p$  统计量在训练的 $RSS$ 中加入了 $2d \hat\sigma^ 2$ 的惩罚项，以调整训练误差容易低估测试误差的事实。显然，惩罚随着模型中预测变量个数的增加而增加，这意在针对训练 $RSS$ 的相应减少进行调整。
        
        可以说明，如果( 6.2 )中的 $\hat\sigma^ 2$ 是 $\sigma^ 2$ 的无偏估计，则 $C_p$ 是测试 $MSE$ 的无偏估计。
        
        因此，对于测试误差较小的模型，$C_p$ 统计量往往取较小的值，所以在确定一组模型中哪一组最好时，我们选择 $C_p$ 值最小的模型。
        
    
    - AIC 标准是为一大类按最大似然拟合的模型定义的。 对于具有高斯误差的模型 (6.1)，最大似然和最小二乘是一回事。这种情况下 AIC 为
        
        $$
        AIC=\dfrac{1}{n}\left(\mathrm{RSS}+2d\hat{\sigma}^2\right)
        $$
        
        为了简单起见，我们省略了不相关的常数。因此对于最小二乘模型，$C_p$ 和 AIC是相称的
        
    
    - BIC 源于贝叶斯观点，但最终看起来也类似于 $C_p$ (和 AIC )。对于具有 $d$ 个预测变量的最小二乘模型，BIC 为
        
        $$
        \mathrm{BIC}=\dfrac{1}{n}\left(\mathrm{RSS}+\log(n)d\hat{\sigma}^2\right)\tag{6.3}
        $$
        
        与 $C_p$ 一样，对于一个具有较低测试误差的模型，BIC 会倾向于取一个较小的值，因此一般选择 BIC 值最小的模型。注意，BIC 将 $C_p$ 使用的 $2d \hat\sigma^ 2$ 替换为 $log ( n ) d \hat\sigma^ 2$ 项，其中 $n$ 为观测数。由于对于任意 $n > 7$，$log n > 2$，BIC 统计量通常会对变量较多的模型施加较重的惩罚，从而导致选择比 $C_p$ 更小的模型
        
    
    - 调整后的 $R^2$
        
        $$
        \operatorname{Adjud}R^2=1-\dfrac{\operatorname{RSS}/(n-d-1)}{\operatorname{TSS}/(n-1)}\tag{6.4}
        $$
        
        越大的调整 $R^2$ 会有越小的测试误差。
        
        最大化调整后的 $R^2$ 等价于最小化 $\frac{RSS} {n - d - 1}$。虽然 $RSS$ 总是随着模型中变量个数的增加而减小，但由于分母中存在 $d$， $\frac{RSS} {n - d - 1}$ 可能增大也可能减小
        

- 验证和交叉验证
    
    > 作为上述方法的替代，我们可以直接使用第 5 章讨论的验证集和交叉验证方法估计测试误差。我们可以计算每个模型的验证集误差或交叉验证误差，然后选择得到的估计测试误差最小的模型。相对于AIC、BIC、$C_p$ 和调整后的 $R^2$，该方法的优势在于提供了对测试误差的直接估计，并且对真实的基础模型做出了更少的假设。
    > 
    

## 收缩方法（*****shrinkage methods*****)

> 子集选择方法涉及使用最小二乘法来构建包含预测变量子集的线性模型。 作为替代方案，我们可以使用一种技术来约束或正则化系数估计，或者等效地，将系数估计缩小到零，从而构建一个包含所有 $p$ 个预测变量的模型。 这样的约束是否会改善拟合可能不是很明显，但事实证明缩小系数估计可以显着减少它们的方差。 将回归系数缩小到零的两种最著名的技术是岭回归（ridge regression）和套索（lasso）。
> 

### 岭回归

> 最小二乘法通过最小化 $\mathrm{RSS}=\sum_{i=1}^{n}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{i j}\right)^{2}$ 来拟合模型，岭回归与此类似
> 

---

岭回归通过最小化

$$
\sum\limits_{i=1}^n\left(y_i-\beta_0-\sum\limits_{j=1}^p\beta_jx_{ij}\right)^2+\lambda\sum\limits_{j=1}^p\beta_j^2=\text{RSS}+\lambda\sum\limits_{j=1}^p\beta_j^2\tag{6.5}
$$

$\lambda \ge0$，称为调和参数（tuning parameter）需要单独确定。

与最小二乘法一样，岭回归通过使 $RSS$ 变小来寻求对数据拟合较好的系数估计。然而，第二项 $\lambda\sum\limits_{j=1}^p\beta_j^2$ 称为收缩惩罚，当 $\beta_1,\dots,\beta_p$ 趋于零时，收缩惩罚会变小，因此它具有将 $\beta_j$ 的估计值收缩到零的效果。

当 $\lambda=0$ 时，惩罚项不起作用，岭回归将产生最小二乘估计。然而，当$\lambda→∞$ 时，收缩惩罚的影响越来越大，岭回归系数估计值将趋近于零。与最小二乘只产生一组系数估计值不同，岭回归对每一个 $\lambda$ 值都会产生一组不同的系数估计值 $\hat\beta_{\lambda}^ R$ ，所以选择合适的 $\lambda$ 很重要。

![Untitled](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled%201.png)

随着 $\lambda$ 的增大，所有系数的估计都趋近于零，对应不含变量的原始模型

标准最小二乘估计是尺度等变（scale equivariant）的，即无论第 $j$ 个预测器如何缩放，$X_j \hat\beta_ j$ 将保持不变。相比之下，当一个给定的预测变量乘以一个常数时，岭回归系数估计值可以发生实质性的变化。通过 式（6.5）$X_j\hat\beta_{\lambda}^R$ 不仅依赖 $\lambda$ 还依赖第 $j$ 个预测值的缩放，甚至依赖其他预测值的缩放

 因此，当预测值标准化后，最好使用岭回归，使用公式（6.6）来使所有预测值在同一尺度下：

$$
\tilde{x}_{ij}=\dfrac{x_{ij}}{\sqrt{\frac1n\sum_{i=1}^n(x_{ij}-\overline{x}_j)^2}}\tag{6.6}
$$

分母为第 $j$ 个预测变量的估计标准差。因此，所有标准化的预报因子都会有一个标准差。

---

- 为什么岭回归比最小二乘有提升

> 岭回归相对于最小二乘的优势源于偏差-方差权衡
> 

如下图随着 $\lambda$ 增大，岭回归拟合的灵活度减小，导致方差减小但偏差增大。

![平方偏差（黑线），方差（绿线），测试均方误差（紫线）](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled%202.png)

平方偏差（黑线），方差（绿线），测试均方误差（紫线）

一般来说，在响应和预测变量之间的关系接近线性的情况下，最小二乘估计会有较低的偏差，但可能有较高的方差。这意味着训练数据的微小变化可以引起最小二乘系数估计值的较大变化。特别地，当变量个数 $p$ 几乎和观测值个数 $n$ 一样大时，最小二乘估计会出现极大的变化。如果 $p > n$，那么最小二乘估计甚至没有唯一解，而岭回归仍然可以通过小幅增加偏差换取大幅度降低方差来表现良好。因此，**岭回归在最小二乘估计具有高方差的情况下效果很好**。

与最佳子集选择相比，岭回归也具有很大的计算优势，前者需要在 $2^p$ 个模型中进行搜索。正如我们前面所讨论的，即使对于适中的 $p$ 值，这样的搜索在计算上也是不可行的。相比之下，对于任意的 $\lambda$ 值，岭回归只需要一个单一的模型，并且模型拟合过程可以相当快速地执行。

---

### 套索（lasso）

> 岭回归的最终模型中将包含所有的 $p$ 个预测因子。（ 6.5 ）式 中的惩罚项$\lambda\sum\limits_{j=1}^p\beta_j^2$ 会使所有系数都向零收缩，但不会使其中任何一个系数恰好为零（ 除非 $\lambda=∞$ ）
这可能不是预测精度的问题，但在变量数目 $p$ 很大的情况下，这会给模型解释带来挑战。
> 

套索是岭回归的替代方法，克服了这一缺点。套索通过最小化（ 6.7）式来估计系数

$$
\sum\limits_{i=1}^n\left(y_i-\beta_0-\sum\limits_{j=1}^p\beta_jx_{ij}\right)^2+\lambda\sum\limits_{j=1}^p|\beta_j|=\mathrm{RSS}+\lambda\sum\limits_{j=1}^p|\beta_j|\tag{6.7}
$$

与岭回归一样，套索将系数估计值向零收缩。但在套索的情况下，当 $\lambda$ 很大时， $\ell_1$ 惩罚可以迫使部分系数估计值恰好等于零。

![根据 $\lambda$ 的取值，套索可以产生一个包含任意多个变量的模型。](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled%203.png)

根据 $\lambda$ 的取值，套索可以产生一个包含任意多个变量的模型。

---

- 岭回归与套索的另一形式
- 套索的变量选择性质
- 岭回归和套索的简单例子
- 岭回归和套索的贝叶斯解释

### 调和参数的选择

> 交叉验证可以选择合适的调和参数
> 

---

## 降维方法（***************************dimension reduction methods***************************)

> 我们现在探索一类方法来转换预测变量，然后使用转换后的变量建立最小二乘模型。我们将这些技术称为降维方法。
> 

令 $Z_1,Z_2,\dots,Z_M$ 表示初始 $p$ 个预测变量的线性组合（$M < p$）则

$$
Z_m=\sum\limits_{j=1}^p\phi_{jm}X_j\tag{6.16}
$$

$\phi_{jm}$ 为常数。代入（6.17）线性模型使用最小二乘拟合：

$$
y_i=\theta_0+\sum\limits_{m=1}^M\theta_m z_{im}+\epsilon_i,\quad i=1,\ldots,n\tag{6.17}
$$

如果 $\phi_{jm}$ 选择合适，使用最小二乘法拟合( 6.17 )比使用最小二乘法拟合( 6.1 )可以得到更好的结果。

所有的降维方法都分两步进行。首先，变换后的预测因子得到$Z_1,Z_2,\dots,Z_M$。第二，模型使用这 $M$ 个预测因子拟合模型。但是，$Z_1,Z_2,\dots,Z_M$ 的选择，或者等价地，$\phi_{jm}$ 的选择可以通过不同的方式实现。在本章中，我们将考虑两种方法：主成分（principal components）和偏最小二乘（partial least squares）。

### 主成分回归（principal components regression）

> 主成分分析( principal component analysis，PCA )是一种从大量变量中提取低维特征的流行方法。将在第12章更详细地讨论了PCA作为无监督学习的工具。在这里我们描述它作为一种降维技术用于回归
> 

---

- 主成分分析概述

PCA是一种对$n × p$ 维数据矩阵$X$ 进行降维的技术

数据的第一个主成分方向是观测值变化最大的方向；对于PCA还有另外一种解释：第一个主成分向量决定了与数据尽可能接近的直线。

第二主成分 $Z_2$ 是与 $Z_1$ 不相关的变量的线性组合，且在此约束下方差最大。$Z_1$ 与 $Z_2$ 的零相关条件等价于方向必须与第一个主成分方向垂直或正交。

如果我们有其他的预测因子，如人口年龄、收入水平、受教育程度等，则可以构造额外的成分。它们在与前面的成分不相关的约束下，依次最大化方差。

---

- 主成分回归方法（ principal components regression (PCR) ）

尽管 PCR 提供了一种使用 $M < p$ 个预测变量进行回归的简单方法，但它不是一种特征选择方法。这是因为回归所用的 $M$ 个主成分中的每一个都是原始特征中所有 $p$ 个的线性组合。所以 PCR 更贴近岭回归。

在PCR中，主成分个数M通常通过交叉验证来选择。

![Untitled](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled%204.png)

在进行主成分分析时，一般建议在生成主成分之前，使用( 6.6 )式对每个预测因子进行标准化处理。这种标准化保证了所有变量在同一尺度上。在没有标准化的情况下，高方差变量会倾向于在得到的主成分中发挥更大的作用，而变量所衡量的尺度最终会对PCR模型产生影响

### 偏最小二乘 （Partial Least Squares， PLS）

> PCR是无监督的，因为不需要响应变量来调整成分的选择。因此有一个缺点：不能保证最好地解释预测因子的方向也是预测反应的最佳方向。
> 

- 什么是偏最小二乘法？
    
    偏最小二乘法是一种监督方法。与 PCR 一样，PLS 是一种降维方法，它首先识别出一组新的特征 $Z_1,Z_2,\dots,Z_M$ 是原始特征的线性组合，然后利用这 $M$ 个新特征通过最小二乘建立线性模型。但与 PCR 不同的是，PLS 以一种有监督的方式识别这些新特征，即利用响应 $Y$ 来识别，新特征不仅能很好地近似旧特征而且与响应变量相关
    

---

- 如何计算PLS 方向？
    
    对 $p$ 个预测变量进行标准化处理后，PLS 通过设定式( 6.16 )中的每个 $\phi_{j1}$ 等于 $Y$ 对 $X_j$ 的简单线性回归的系数，来计算第一个方向 $Z_1$ 。因此，该系数与 $Y$ 和 $X_j$ 的相关性成正比。因此，在计算 $Z_m=\sum\limits_{j=1}^p\phi_{jm}X_j$ 时，PLS 对与响应关系最强的变量赋予最高权重。
    
    为了识别第二个 PLS 方向，我们首先对 $Z_1$ 中的每个变量进行调整，将每个变量对 $Z_1$ 进行回归并取残差。这些残差可以解释为未被第一个 PLS 方向解释的剩余信息。然后我们使用这个正交化的数据计算 $Z_2$ ，其方式与基于原始数据计算的 $Z_1$  完全相同。这种迭代方法可以重复 $M$ 次来识别多个 PLS 成分 $Z_1,\dots,Z_M$。
    

---

与PCR一样，PLS中使用的偏最小二乘方向数M是一个调节参数，通常通过交叉验证来选择。在进行PLS之前，我们通常对预测因子和响应进行标准化。

在实际应用中，它往往表现不如岭回归或PCR。PLS的有监督降维在减小偏差的同时，还具有增大方差的潜力，使得PLS相对于PCR的总体效益是水涨船高。

---

## 高纬度考虑（***********************considerations in high dimensions***********************)

### 高维数据

> 大多数传统的用于回归和分类的统计技术都是针对观测数 $n$ 远大于特征数 $p$ 的低维环境。包含比观测值 $n$ 更多特征 $p$ 的数据集通常被称为高维数据。
> 

---

### 高纬度会带来什么问题 ？

> 我们考察最小二乘法在高维数据中的情况，其余经典统计方法也同理。
> 

当特征数 $p$ 大于或等于观测数 $n$ 时，无法进行第3章所述的最小二乘(或者说,不应该)。原因很简单：不管特征和响应之间是否真的存在关系，最小二乘都会产生一组系数估计值，导致模型完美拟合数据，使得残差为零。

假设 $p=1$ ，$n=20$ 时

![Untitled](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled%205.png)

最小二乘回归线不能很好的拟合数据，但它会尽可能去近似这 20 个观测值。

![Untitled](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled%206.png)

另一方面，$p=1,n=2$ 时，无论这些观测值的大小如何，回归线都会准确地拟合数据。这是有问题的，因为这个完美的拟合几乎肯定会导致数据的过拟合。

![Untitled](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled%207.png)

模型的 $R^2$ 随着模型包含的特征数量的增加而增加到 1，相应地训练集 $MSE$ 随着特征数量的增加而减小到 0，即使特征与响应变量完全无关。

这说明，在分析变量较多的数据集时，需要格外小心，并始终在独立测试集上评估模型性能。

---

### 在高维回归

> 许多弱灵活性的最小二乘模型的方法，如向前逐步选择、岭回归、lasso和主成分回归等，对于在高维情况下进行回归特别有用。本质上，这些方法通过使用比最小二乘更不灵活的拟合方法来避免过度拟合。
> 

![Untitled](Linear%20Model%20Selection%20and%20Regularization%207568f4f7cdd84dbeb1341fe675652ca1/Untitled%208.png)

用 $n = 100$ 个观察值和三个特征 $p$ 值进行 lasso。在 p 个特征中，有 20 个与响应相关。箱线图显示了使用 ( 6.7 ) 中调优参数 $\lambda$  的三个不同值的测试 MSE 。为了便于解释，横坐标为自由度，对于套索而言，这只是估计的非零系数的个数。当 $p = 20$ 时，正则化量最小的测试 MSE 最小。当 $p = 50$ 时，有大量正则化时达到最低的测试 MSE 。当 $p = 2$ 时，由于 2 000 个特征中只有 20 个与结果真正相关，因此无论正则化的多少，lasso的表现都很差。

[上图](Linear%20Regression%20a99b01f6ea794d299cbfbfa76cf73d36.md) 突出了三个重要的点：( 1 ) 正则化或收缩在高维问题中起着关键作用，( 2 ) 适当的调节参数选择对于良好的预测性能至关重要，( 3 ) 随着问题(即特征或预测因子的数量)的维数增加，测试误差往往会增加，除非额外的特征与响应真正相关。

允许收集数千或数百万个特征的测量是一把双刃剑：如果这些特征实际上与手头的问题相关，它们可以导致改进的预测模型，但如果这些特征不相关，则会导致更糟糕的结果。即使它们是相关的，对它们的系数进行拟合所带来的方差也可能大于它们带来的偏差的减少。

---

### 高维的解释

> 我们了解了多重共线性的概念，即回归中的变量可能彼此相关。在高维设定下，多重共线性问题是极端的：模型中的任何变量都可以写成模型中所有其他变量的线性组合。本质上，这意味着我们永远无法确切地知道哪些变量(如果有的话)真正对结果有预测作用，我们永远无法确定在回归中使用的最佳系数。至多，我们可以希望将大的回归系数分配给与真正预测结果的变量相关的变量。
> 

在高维情况下，不应该在训练数据上使用误差平方和、p值、$R^2$ 统计量或其他传统度量作为模型拟合良好的证据。重要的是在独立的测试集上报告结果，或者交叉验证错误。