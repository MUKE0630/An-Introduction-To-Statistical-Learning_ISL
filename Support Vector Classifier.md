# Support Vector Classifier

支持向量机是在本章第一节介绍的一种简单直观的最大间隔分类器的推广。尽管它是优雅和简单的，但我们会看到，不幸的是，这个类不能应用于大多数数据集，因为它要求类是线性边界可分的。在第二节中，我们介绍了支持向量类，它是最大间隔类的扩展，可以应用于更广泛的情况。第三节介绍了支持向量机，它是支持向量分类机的进一步扩展，以适应非线性的类边界。支持向量机适用于二分类问题，即存在两类；第四节讨论了支持向量机扩展到两类以上的情况。在第五节中我们讨论了支持向量机与其它统计方法如 logistic 回归之间的密切联系。

人们通常将最大间隔分类、支持向量分类和支持向量机统称为"支持向量机"。为了避免混淆，我们将在本章仔细区分这三个概念。

---

# 最大间隔分类（*Maximal Margin Classier*）

### 超平面

在 $p$ 维空间中，超平面是一个$p − 1$ 维的平面仿射子空间。例如，在二维空间中，超平面是一个一维的子空间——也就是一条线。 在三维中，超平面是一个二维子空间——即平面。 在 $p > 3$  维中，很难将超平面可视化，但子空间 $(p − 1)$  维的概念仍然适用。

在二维空间上，超平面的数学定义为

$$
\beta_0+\beta_1X_1+\beta_2X_2=0\tag{9.1}
$$

对于( 9.1 )成立的任意 $X = ( X_1 ,X_2)^T$，都是超平面上的一点。

方程9.1可以很容易地推广到 $p$ 维情形

$$
\beta_0+\beta_1X_1+\beta_2X_2+\dots+\beta_pX_p=0\tag{9.2}
$$

假设 $X$ 不满足（9.2）例如

$$
\beta_0+\beta_1X_1+\beta_2X_2+\dots+\beta_pX_p>0\tag{9.3}
$$

则 $X$ 位于超平面的一侧。因此我们可以将超平面看作是将 $p$ 维空间分成两半。通过计算( 9.2 )的左手边的符号，我们可以很容易地确定一个点位于超平面的哪一边。

---

### 使用超平面进行分类

现在假设我们有一个$n × p$ 的数据矩阵$\boldsymbol X$，它由 $p$ 维空间中的 $n$ 个训练观测组成:

$$
x_1=\begin{pmatrix}x_{11}\\ \vdots\\ x_{1p}\end{pmatrix},\ldots,x_n=\begin{pmatrix}x_{n1}\\ \vdots\\ x_{np}\end{pmatrix}\tag{9.5}
$$

这些观测分为两类——即 $y_1,\dots,y_n∈\{ -1,1 \}$，其中 -1 代表一类，1 代表另一类。我们也有一个测试观测，观测特征的 $p$ 维向量 $x^* = ( x_1^ *,\dots, x^ * _p)^T$。我们的目标是开发一个基于训练数据的分类器，它将使用测试观测的特征测量来正确分类。

假设可以构造一个超平面，根据训练观测的类别标签将它们完美地分开。这 3 个超平面的例子如图 9.2 左图所示。我们可以将来自蓝色类的观测值标记为$y_i = 1$ ，来自紫色类的观测值标记为 $y_i = - 1$。则分离超平面具有如下性质

$$
\beta_0+\beta_1X_1+\beta_2X_2+\dots+\beta_pX_p>0\space \text{if}\space y_i=1 \\\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}<0\space\textrm{if}\space y_i=-1 \tag{9.6}
$$

![图9.2 .左：有两类观测值，分别用蓝色和紫色表示，每个观测值都有两个变量的测量值。在许多可能的情况下，三个分离超平面以黑色显示。右：一个分离超平面用黑色表示。蓝色和紫色网格表示一个类基于这个分离超平面做出的决策规则：落在网格蓝色部分的测试观测将被分配给蓝色类，落在网格紫色部分的测试观测将被分配给紫色类。](Support%20Vector%20Classifier%202cc2b25a454b4bbaaa6a31ab18953bc1/Untitled.png)

图9.2 .左：有两类观测值，分别用蓝色和紫色表示，每个观测值都有两个变量的测量值。在许多可能的情况下，三个分离超平面以黑色显示。右：一个分离超平面用黑色表示。蓝色和紫色网格表示一个类基于这个分离超平面做出的决策规则：落在网格蓝色部分的测试观测将被分配给蓝色类，落在网格紫色部分的测试观测将被分配给紫色类。

如果存在一个分离超平面，我们可以用它来构造一个非常自然的类：一个测试观测根据它所在超平面的哪一边被分配一个类。图9.2的右边面板显示了这样一个类的例子。即对检验观测值 $x ^*$ 按 $f ( x ^* ) = \beta_0 + \beta_1x_1^ * + \beta_2x_2^ * + · · + \beta_px^ * _p$ 的符号进行分类。若 $f ( x^ * )$ 为正，则将测试观测值分配给类1，若 $f ( x ^* )$ 为负，则将其分配给类 -1 。我们还可以利用 $f ( x ^* )$ 的大小，如果 $f ( x^ * )$ 远离零，则意味着 $x ^*$ 远离超平面，从而可以确定 $x ^*$ 的类分配。另一方面，如果 $f ( x ^* )$ 接近于0，则 $x ^*$ 位于超平面附近，因此我们对 $x  ^*$ 的类分配不太确定。毫不奇怪，正如我们在图9.2中看到的，一个基于分离超平面的类会导致一个线性决策边界。

---

### 最大间隔类

一般而言，如果我们的数据可以用一个超平面完美分离，那么实际上会存在无穷多个这样的超平面。这是因为给定的分离超平面通常可以向上或向下移动一个微小的位，或者旋转，而不与任何观测接触。3个可能的分离超平面如图9.2左图所示。为了构造一个基于分离超平面的分类器，我们必须有一个合理的方法来决定使用哪些有限的可能的分离超平面。

一个自然选择是最大间隔超平面(也称为最优分离超平面)，它是距离训练观测值最远的分离超平面。也就是说，我们可以计算每个训练观测到给定分离超平面的(垂直)距离；最小的这种距离是从观测到超平面的最小距离，称为间隔。最大间隔超平面是间隔最大的分离超平面|，即距离训练观测值最小距离最远的超平面。虽然最大间隔分类往往是成功的，**但当 $p$ 很大时，它也会导致过拟合。**

观察图9.3，我们看到三个训练观测值与最大间隔超平面等距，并且位于指示间隔宽度的虚线上。这三个观测被称为支持向量，因为它们是 $p$ 维空间（ 图9.3中, $p = 2$ ）中的向量，并且它们支持最大间隔超平面，如果这些点稍微移动，那么最大间隔超平面也会移动。有趣的是，最大间隔超平面直接依赖于支持向量，而不依赖于其他观测值：如果观测值的移动不会导致其越过间隔设定的边界，那么向其他任何观测值移动都不会影响分离超平面。

---

### 最大间隔类的构造

我们现在考虑基于 $n$ 个训练观测值 $\begin{aligned}x_1,\ldots,x_n\in\mathbb{R}^p\end{aligned}$ 构造最大间隔超平面的任务，及相关类标  $y_1,\dots,y_n\in\{ -1，1 \}$ 。简言之，最大间隔超平面就是优化以下问题的解

$$
\underset{\beta_0,\beta_1,\ldots,\beta_p,M}{\text{maximize}}M\tag{9.9}
$$

$$
\text{subject to}~\sum\limits_{j=1}^{p}\beta_j^2=1\tag{9.10}
$$

$$
y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip})\geq M \space \forall  i=1,\ldots,n\tag{9.11}
$$

首先，( 9.11 )中的约束保证每个观测都在超平面的正确一侧，前提是 $M$ 是正的。

其次，注意到式( 9.10 ) 实际上并不是对超平面的约束，因为如果 $\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}=0$ 是一个超平面，那么对任意 $k \ne 0$，$k(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip})=0$ 也是一个超平面。然而 ( 9.10 ) 为( 9.11 ) 增加了意义，可以证明，在此约束下，第 $i$ 个观测到超平面的垂直距离为

$$
y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip})
$$

因此，约束条件( 9.10 )和( 9.11 )保证了每个观测都在超平面的正确一侧，且距离超平面至少有一段距离 $M$。因此，$M$ 表示超平面的边界，优化问题选择$\beta_0,\beta_1,\dots,\beta_p$ 最大化 $M$ 。这正是最大间隔超平面的定义。

---

### 不可分离的情况

如果存在分离超平面，最大间隔分类是一种非常自然的执行分类的方法。然而，在许多情况下不存在分离超平面，因此不存在最大间隔类。此时，优化问题( 9.9 ) - ( 9.11 )在$M > 0$ 时无解。实例如图9.4所示。在这种情况下，我们无法准确地将两类分开。然而，正如我们在下一节中看到的，我们可以扩展分离超平面的概念，以便使用所谓的软间隔来开发一个几乎可以分离类的超平面。最大间隔分类器对不可分情况的泛化称为支持向量分类器。

![图9.4 .有两类观察值，用蓝色和紫色表示。在这种情况下，两个类不能被一个超平面分离，因此最大间隔类不能被使用。](Support%20Vector%20Classifier%202cc2b25a454b4bbaaa6a31ab18953bc1/Untitled%201.png)

图9.4 .有两类观察值，用蓝色和紫色表示。在这种情况下，两个类不能被一个超平面分离，因此最大间隔类不能被使用。

---

# 支持向量分类器（*Support Vector Classiers*）

### 支持向量分类器概述

在图9.4中，我们看到属于两类的观测值不一定能被超平面分离。事实上，即使存在分离超平面，也存在分类可能不理想的情况。一个基于分离超平面的分类器必然会完美地分类所有的训练观测，这会导致对单个观测值的敏感性。实例如图9.5所示。在图9.5的右边面板中加入单个观测，导致最大间隔超平面发生剧烈变化。由此得到的最大间隔超平面并不令人满意—— 一方面，它只有很小的间隔。这是有问题的，因为正如前面所讨论的，观测到超平面的距离可以被看作是我们相信观测被正确分类的一个度量。此外，最大间隔超平面对单个观测值的变化极其敏感的事实表明它可能过拟合训练数据。

![图9.5 .左：两类观测值分别以蓝色和紫色显示，以及最大间隔超平面。右：增加了一个额外的蓝色观测，导致以实线表示的最大间隔超平面发生了剧烈的偏移。虚线表示在没有这个附加点的情况下得到的最大间隔超平面。](Support%20Vector%20Classifier%202cc2b25a454b4bbaaa6a31ab18953bc1/Untitled%202.png)

图9.5 .左：两类观测值分别以蓝色和紫色显示，以及最大间隔超平面。右：增加了一个额外的蓝色观测，导致以实线表示的最大间隔超平面发生了剧烈的偏移。虚线表示在没有这个附加点的情况下得到的最大间隔超平面。

在这种情况下，我们可能愿意考虑一个基于超平面的类，该超平面不能完美地分离两个类

- 对个体观测值具有更强的鲁棒性，且
- 较好的分类了大部分的训练观测值。

也就是说，为了更好地对剩余的观测值进行分类，对少数训练观测值进行误分类是值得的。实例如图9.6左手面板所示。大部分观测值都在边界的正确一侧。然而，有一小部分观测值位于边界的错误一侧。

![图9.6 .左：一个支持向量类器拟合到一个小数据集。超平面显示为实线，边缘显示为虚线。紫色观察：观值值3，4，5，6位于边界正确一侧，观测2位于边界，观测1位于边界错误一侧。蓝色观察：观察值7，10在边缘正确侧，观察9在边缘，观察8在边缘错误侧。没有观测在超平面的错误一侧。右：与左面板相同，增加两个点，11和12。这两个观测值位于超平面的错误一侧和边界的错误一侧。](Support%20Vector%20Classifier%202cc2b25a454b4bbaaa6a31ab18953bc1/Untitled%203.png)

图9.6 .左：一个支持向量类器拟合到一个小数据集。超平面显示为实线，边缘显示为虚线。紫色观察：观值值3，4，5，6位于边界正确一侧，观测2位于边界，观测1位于边界错误一侧。蓝色观察：观察值7，10在边缘正确侧，观察9在边缘，观察8在边缘错误侧。没有观测在超平面的错误一侧。右：与左面板相同，增加两个点，11和12。这两个观测值位于超平面的错误一侧和边界的错误一侧。

一个观测不仅可以位于边界的错误一侧，也可以位于超平面的错误一侧。事实上，当不存在分离超平面时，这种情况是不可避免的。超平面错误一侧的观测值对应支持向量分类器误分类的训练观测值。图9.6的右边面板说明了这种情况。

---

### 支持向量分类器的细节

支持向量类根据其所在超平面的哪一边对测试观测进行分类。超平面被选择来正确地将大部分训练观测分为两类，但是可能会误分类少数观测。它是优化问题的解

$$
\begin{array}{c}\text{maximize}\\ \beta_0,\beta_1,...,\beta_p,\epsilon_1,...,\epsilon_n,M\end{array}M\tag{9.12}
$$

$$
\text{subject to}~\sum_{j=1}^{p}\beta_j^2=1\tag{9.13}
$$

$$
y_i(\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip})\geq M(1-\epsilon_i)\tag{9.14}
$$

$$
\epsilon_i\geq0,\sum\limits_{i=1}^n\epsilon_i\leq C\tag{9.15}
$$

$C$ 为非负调节参数。如式( 9.11 )，$M$ 为边距的宽度，我们寻求使这个数尽可能大。在( 9.14 )中，$\epsilon_1,\dots,\epsilon_n$ 是松弛变量（*slack variables*），允许个体观测值位于边界或超平面的错误一侧，暂时不会详细地解释它们。一旦我们解出了( 9.12 ) - ( 9.15 )，我们就像前面一样，通过简单地确定一个测试观测 $x ^*$ 位于超平面的哪一边来对它进行分类。即根据 $f ( x ^* ) = \beta_0 + \beta_1x_1^ * + \dots+ \beta_px^ *_ p$ 的符号对测试观测值进行分类。

问题( 9.12 ) - ( 9.15 )看起来很复杂，但可以通过下面的一系列简单观察来洞察它的行为。首先，松弛变量 $\epsilon_i$ 告诉我们第 $i$  个观测位于何处，相对于超平面和相对于边缘。如果 $\epsilon_i = 0$，则第 $i$ 个观测在边界的正确一侧，正如我们在9.1. 4节中看到的。如果 $\epsilon_i > 0$，则第 $i$ 个观测位于边界的错误一侧，我们称第 $i$ 个观测违反了边界。如果 $\epsilon_i > 1$，则它在超平面的错误的一边上。

我们现在考虑调节参数 $C$ 的作用。在( 9.15 )中，$C$ 限制了 $\epsilon_i$ 的和，因此它决定了违反边界(和到超平面)的数量和严重程度。我们可以把 $C$ 看作是 $n$ 个观测值可以违反边界的预算。如果 $C = 0$，则不存在违反边界的预算，并且必须满足 $\epsilon_1 = · · · = \epsilon_n = 0$，此时( 9.12 ) - ( 9.15 )等价于最大间隔超平面优化问题( 9.9 ) - ( 9.11 ) 。（ 当然,只有当两个类是可分的时,最大间隔超平面才存在）。对于 $C > 0$，不超过 $C$ 个观测可以在超平面的错误边上，因为如果一个观测在超平面的错误边上，那么 $\epsilon_i > 1$，并且( 9.15 )要求 $\sum_{i=1}^{n}\epsilon_{i}\leq C\text{}$。随着预算 $C$ 的增加，我们对违反边界的容忍度增加，因此边界会变宽。反之，随着 $C$ 的减小，我们对中对违反的容忍度降低。如图9.7所示

![图9.7 利用( 9.12 ) - ( 9.15 )中调节参数 $C$ 的四个不同值构造了一个支持向量分类器。左上面板使用 $C$ 值最大，右上、左下、右下面板使用C值较小。当 $C$ 很大时，则对位于边界错误一侧的观测值有很高的容忍度，因此边界会很大。随着 $C$ 的减小，观测值落在边界错误一侧的容忍度减小，边界变窄。](Support%20Vector%20Classifier%202cc2b25a454b4bbaaa6a31ab18953bc1/Untitled%204.png)

图9.7 利用( 9.12 ) - ( 9.15 )中调节参数 $C$ 的四个不同值构造了一个支持向量分类器。左上面板使用 $C$ 值最大，右上、左下、右下面板使用C值较小。当 $C$ 很大时，则对位于边界错误一侧的观测值有很高的容忍度，因此边界会很大。随着 $C$ 的减小，观测值落在边界错误一侧的容忍度减小，边界变窄。

**支持向量分类器的决策规则仅基于训练观测值（支持向量）的一个潜在的小子集，这意味着它对远离超平面的观测值的行为相当鲁棒**。这一性质与我们在前面章节中看到的其他一些分类方法，例如线性判别分析，是不同的。回想一下，LDA分类规则取决于所有观测点的均值以及使用所有观测值计算的类内协方差矩阵。相比之下，与LDA不同，逻辑回归对远离决策边界的观测值的敏感度很低。事实上，我们将在第五节看到，支持向量分类机和逻辑回归是密切相关的。

---

# 支持向量机（*Support Vector Machines*）

### 具有非线性决策边界的分类

如果两个类之间的边界是线性的，则支持向量分类机是在二分类环境下进行分类的一种自然方法。然而，在实践中我们有时会面临非线性的类边界。例如，考虑图9.8左边面板中的数据。显然，一个支持向量类或任何一个线性类在这里都会表现不佳。事实上，图9.8右边面板中显示的支持向量分类器在这里是无用的。

![图9.8  左：观测值分为两类，它们之间存在非线性边界。右：支持向量分类器寻求线性边界，因此表现很差。](Support%20Vector%20Classifier%202cc2b25a454b4bbaaa6a31ab18953bc1/Untitled%205.png)

图9.8  左：观测值分为两类，它们之间存在非线性边界。右：支持向量分类器寻求线性边界，因此表现很差。

在支持向量分类器的情况下，通过使用预测变量的二次、三次甚至更高阶的多项式函数来扩大特征空间。例如，不使用 $p$ 个特征构造支持向量分类器，我们可以使用 $2p$ 个特征来代替支持向量分类器

$$
X_1,X_1^2,X_2,X_2^2,\dots,X_P,X_P^2
$$

（9.12）-（9.15）变成

$$
\begin{array}{c}\text{maximize}\\ \beta_0,\beta_1,...,\beta_p,\epsilon_1,...,\epsilon_n,M\end{array}M\\\mathrm{subject~to~}y_{i}\left(\beta_{0}+\sum_{j=1}^{p}\beta_{j1}x_{i j}+\sum_{j=1}^{p}\beta_{j2}x_{i j}^{2}\right)\geq M(1-\epsilon_{i})\\\sum\limits_{i=1}^n\epsilon_i\leq C,\epsilon_i\geq0,\sum\limits_{j=1}^p\sum\limits_{k=1}^2\beta_{jk}^2=1\tag{9.16}
$$

在扩大的特征空间中，由( 9.16 )式得到的决策边界实际上是线性的。但在原始特征空间中，决策边界的形式为 $q ( x ) = 0$，其中 $q$ 是二次多项式，其解一般是非线性的。另外，人们可能希望用高阶多项式项来扩大特征空间，或者用 $X_jX_{j'}$ ，$j \ne j'$ 形式的交互项来扩大特征空间。或者，可以考虑预测因子的其他函数而不是多项式。扩大特征空间，应该小心谨慎，否则最终会得到大量的特征，那么计算量将变得不可控。我们接下来提出的支持向量机允许我们以一种高效计算的方式扩大支持向量分类机使用的特征空间。

---

### 支持向量机

> 支持向量机（ *support vector machine*，SVM ）是支持向量分类器的一种扩展，是通过核函数以特定的方式扩大特征空间的结果。
> 

支持向量分类问题( 9.12 ) - ( 9.15 )的解只涉及观测值的内积。两个观测值 $x_i,x_{i'}$ 的内积为

$$
\langle x_i,x_{i'}\rangle=\sum\limits_{j=1}^{p}x_{ij}x_{i'j}\tag{9.17}
$$

可以得出

- 线性支持向量分类器可以表示为
    
    $$
    f(x)=\beta_0+\sum_{i=1}^n\alpha_i\langle x,x_i\rangle\tag{9.18}
    $$
    
    式中：有 $n$ 个参数 $\alpha_i$，$i = 1,\dots,n$
    
- 估计参数 $\alpha_1,\dots,\alpha_n$ 和 $\beta_0$ ，我们所需要的是所有训练观测对之间，一共 $\binom{n}{2}$ 个 $\langle x_{i},x_{i'}\rangle\text{}$ 内积。（记号 $\binom{n}{2}$ 表示 $n( n-1) / 2$，并给出一组 $n$ 项之间的成对数量）。

注意到在( 9.18 )中，$\alpha_i$ 仅对解中的支持向量非零，即如果一个训练观测不是支持向量，那么它的 $\alpha_i$ 等于零。因此，如果 $\boldsymbol S$ 是这些支持向量点的集合，我们可以将式( 9.18 )的任意解函数改写为

$$
f(x)=\beta_0+\sum_{i\in\mathcal{S}}\alpha_i\langle x,x_i\rangle\tag{9.19}
$$

总之，在表示线性类 $f ( x )$ 和计算其系数时，我们所需要的都是内积。

现在假设每次内积( 9.17 )出现在( 9.18 )中，或者在计算支持向量分类器的解时，我们用形式的内积的推广来代替它

$$
K(x_i,x_{i'})\tag{9.20}
$$

$K$ 是某种函数，我们将其称为核。核是量化两个观测值相似性的函数。例如，我们可以简单地取

$$
K(x_i,x_{i'})=\sum_{j=1}^{p}x_{ij}x_{i'j}\tag{9.21}
$$

这个函数会让我们回到支持向量分类器。式( 9.21 )被称为线性核，因为支持向量分类器在特征中是线性的；线性核实质上是利用皮尔逊（Pearson）(标准)相关来量化一对观测的相似性。但对于( 9.20 )，我们可以选择另一种形式

$$
K(x_i,x_{i'})=(1+\sum\limits_{j=1}^{p}x_{ij}x_{i'j})^d\tag{9.22}
$$

这被称为 $d$ 次多项式核，其中 $d$ 为正整数。使用这样一个$d > 1$ 的核，而不是标准的线性核( 9.21 )，在支持向量分类机算法中可以得到更加灵活的决策边界。它本质上相当于在一个包含d次多项式的高维空间中构造一个支持向量类，而不是在原始特征空间中。当支持向量分类器与非线性核(如式( 9.22 ) )结合时，得到的分类器称为支持向量机。注意，在这种情况下(非线性)函数具有形式

$$
f(x)=\beta_0+\sum_{i\in\mathcal{S}}\alpha_i K(x,x_i)\tag{2.23}
$$

图9.9的左边面板展示了一个应用于图9.8中非线性数据的多项式核SVM的例子。拟合是线性支持向量分类机的一个实质性改进。当 $d = 1$ 时，SVM退化为本章前面提到的支持向量分类器。

![图9.9  左：将具有3次多项式核的 SVM 应用于图9.8中的非线性数据，得到更合适的决策规则。右：采用径向核（*radial kernel*）的 SVM。在这个例子中，任一核都有能力捕捉决策边界。](Support%20Vector%20Classifier%202cc2b25a454b4bbaaa6a31ab18953bc1/Untitled%206.png)

图9.9  左：将具有3次多项式核的 SVM 应用于图9.8中的非线性数据，得到更合适的决策规则。右：采用径向核（*radial kernel*）的 SVM。在这个例子中，任一核都有能力捕捉决策边界。

式( 9.22 )所示的多项式核是一个可能的非线性核的例子，但替代方案比比皆是。另一种流行的选择是径向核，它采取的形式

$$
K(x_i,x_{i'})=\exp(-\gamma\sum_{j=1}^p(x_{ij}-x_{i'j})^2)\tag{9.24}
$$

$\gamma$ 是一个正的常数。图9.9右边的面板展示了一个在这个非线性数据上具有径向核的 SVM 的例子，它在分离两个类方面也做得很好。

径向核( 9.24 )是如何实际工作的？如果给定的测试观测 $x^ * = ( x_1 ^* ,\dots,x ^*_ p)^T$ 与训练观测 $x_i$ 的欧氏距离很远，那么 $\sum_{j=1}^p(x_j^*-x_{ij})^2)$ 将会很大，因此$K(x_*,x_{i})=\exp(-\gamma\sum_{j=1}^p(x_{j}^*-x_{ij})^2)$将会很小。这意味着在( 9.23 )中，$x_ i$ 在 $f ( x ^* )$ 中几乎不起作用。回想一下，对于测试观测 $x ^*$ 的预测类别标签是基于 $f ( x ^* )$ 的符号。也就是说，距离 $x^ *$ 较远的训练观测值在 $x ^*$ 的预测类别标签中基本不起作用。**这意味着径向核具有非常局部的行为，即只有附近的训练观测对一个测试观测的类标签有影响**。

使用核的好处是什么，而不是简单地使用原始特征的函数来扩大特征空间，如( 9.16 )？一个优点是计算性，即使用核，只需要计算所有 $\binom{n}{2}$ 个不同对 $i,i'$ 的$K( x_i ,x'_i)$。这可以不用显式地扩大的特征空间。这一点很重要，因为在 SVM 的许多应用中，扩大的特征空间太大以至于难以计算。对于某些核，如径向核( 9.24 )，其特征空间是隐式的且是有限维的，因此我们无论如何不能在那里进行计算。

---

# 超过两类的SVMs（*SVMs with More than Two Classes*）

迄今为止，我们的讨论仅限于二分类的情形。我们如何将 SVM 推广到更一般的情况，我们有一些任意数量的类。事实证明，SVM所基于的分离超平面的概念并不适用于两个以上的类。尽管已经提出了一些将SVM扩展到 $K$ 类的建议，但最流行的两种方法是一对一（*one-versus-one*）和一对多（*one-versus-all*）。我们在这里简要讨论这两种方法。

---

### 一对一分类

假设我们希望使用 SVM 进行分类，并且有 $K > 2$ 个类。使用一对一或所有点对构建 $\binom{K}{2}$ 个 SVM，每个 SVM 比较一对类。例如，一个 SVM 可能将第 $k$ 类（编码为+ 1 ）与第 $k'$ 类 （编码为 -1）进行比较。我们使用 $\binom{K}{2}$ 个 SVM 中的每一个对测试观测值进行分类，并统计测试观测值被分配到 $K$ 个类中每个类的次数。最终分类是通过将测试观测分配到 $\binom{K}{2}$ 对分类中最经常分配到的类来执行的。

---

### 一对多分类

一对多方法是在 $K > 2$ 类的情况下应用 SVM 的一种替代方法。我们拟合 $K$ 个 SVM，每次比较其中一个 $K$ 类到剩下的 $K - 1$ 个类。令 $\beta_{0k},\beta_{1k},\dots,\beta_{pk}$ 表示比较第 $k$ 类（编码为+ 1）和其他类（编码为- 1）的 SVM 得到的参数。令 $x^ *$ 表示一个试验观测。我们将观测值分配到 $\beta_{0k},\beta_{1k}x_1^*,\dots,\beta_{pk}x_p^*$ 最大的类中，因为这相当于高度确信测试观测属于第 $k$ 类而不是其他任何类。

---

# 与 Logistic 回归的关系（*Relationship to Logistic Regression*）

我们可以将判别准则( 9.12 ) - ( 9.15 ) }改写为成拟合支持向量类 $f ( X ) = \beta_0 + \beta_1X_1 + · · · + \beta_pX_p$   

$$
\underset{\beta_0,\beta_1,...,\beta_p}{\text{minimize}}\left\{\sum\limits_{i=1}^{n}\max\left[0,1-y_i f(x_i)\right]+\lambda\sum\limits_{j=1}^{p}\beta_j^2\right\}\tag{9.25}
$$

$\lambda$ 是一个非负的调节参数。当 $\lambda$ 较大时 $\beta_1,\dots,\beta_p$ 较小，会容忍更多违反边界的行为，并且会导致低方差但高偏差的分类。当 $\lambda$ 较小时，对边界的违反较少，这相当于一个高方差但低偏差的分类。因此，( 9.25 )中的一个 $\lambda$ 小值相当于( 9.15 )中 $C$ 的一个小值。注意到 ( 9.25 ) 式中的 $\lambda\sum_{j=1}^p\beta_j^2$ 项是岭回归中的惩罚项，在控制支持向量分类器的偏差-方差权衡方面起到了类似的作用。

现在( 9.25 )采取了我们在本书中反复看到的 “损失＋惩罚” 的形式

$$
\underset{\beta_0,\beta_1,\ldots,\beta_p}{\text{minimize}}\left\{L(\mathbf{X},\mathbf{y},\beta)+\lambda P(\beta)\right\}\tag{9.26}
$$

类似的（9.25）的形式可以换为

$$
L(\mathbf{X},\mathbf{y},\beta)=\sum_{i=1}^{n}\max\left[0,1-y_i(\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip})\right]
$$

这被称为铰链损耗（*hinge loss*），如图9.12所示。但事实证明，铰链损失函数与逻辑回归中使用的损失函数密切相关，同样如图9.12所示。

![图9.12  将 SVM 和逻辑回归损失函数作为 $yi( \beta_0 + \beta_1x_{i1} + · · · + \beta_px_{ip})$ 的函数进行比较。当 $yi( \beta_0 + \beta_1x_{i1} + · · · + \beta_px_{ip})$ 大于1时，SVM损失为零，因为这对应于一个位于边界正确一侧的观测值。总体而言，两种损失函数具有非常相似的行为。](Support%20Vector%20Classifier%202cc2b25a454b4bbaaa6a31ab18953bc1/Untitled%207.png)

图9.12  将 SVM 和逻辑回归损失函数作为 $yi( \beta_0 + \beta_1x_{i1} + · · · + \beta_px_{ip})$ 的函数进行比较。当 $yi( \beta_0 + \beta_1x_{i1} + · · · + \beta_px_{ip})$ 大于1时，SVM损失为零，因为这对应于一个位于边界正确一侧的观测值。总体而言，两种损失函数具有非常相似的行为。

支持向量分类机的一个有趣的特点是得到的分类结果中只有支持向量起作用；对切缘正确侧的观察不影响边缘。这是由于对于 $yi( \beta_0 + \beta_1x_{i1} + · · · + \beta_px_{ip})\ge1$ 的观测值，图9.12所示的损失函数恰好为零。与之相比，图9.12所示的逻辑回归的损失函数在任何地方都不完全为零。但对于远离决策边界的观测值来说则非常小。由于损失函数之间的相似性，逻辑回归和支持向量分类机经常给出非常相似的结果。**当类被很好地分离时，SVM往往比逻辑回归表现更好；在有更多的重叠区域中，逻辑回归往往更受青睐。**

当首先引入支持向量分类器和SVM时，认为式( 9.15 )中的调节参数 $C$ 是一个不重要的参数，可以设置为某个默认值，如1。然而，支持向量分类器的公式( 9.25 )表明情况并非如此。调优参数的选择非常重要，它决定了模型在多大程度上低拟合或过拟合数据，如图9.7所示。

我们已经确立了支持向量分类器与逻辑回归等已有的统计方法密切相关。**SVM的独特之处在于它使用核来扩大特征空间以容纳非线性的类边界？这个问题的答案是 ”否“。我们可以使用非线性核来执行逻辑回归或本书中所见的许多其他分类方法，这与书中前几章的一些非线性方法密切相关。**然而，由于历史原因，非线性核的使用在 SVM 中比在逻辑回归或其他方法中更普遍。