# Tree-Based Methods

# 决策树基础

### 回归树

由一个例子来说明树

![Untitled](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b/Untitled.png)

---

### 通过特征空间的分层进行预测

在讨论构建回归树的过程。大致来说，有两个步骤

1. 我们划分预测变量空间——即 $X_1,X_2,\dots,X_p$ 的可能值集合——分成 $J$ 个互不重叠的区域 $R_1,R_2,\dots,R_J$
2. 对于落入区域 $R_j$ 的每一个观测值，我们都做同样的预测，简单来说就是 $R_j$ 中观测值的响应的均值。

例如，假设在步骤1中我们得到两个区域 $R_1$ 和 $R_2$ ，并且第一个区域的训练观测值的响应均值为 10，而第二个区域的训练观测值的响应均值为 20。那么对于给定的观测值 $X = x$，如果 $x∈R_1$ 我们将得到预测值为10，如果 $x∈R_2$ 我们将得到预测值we 20 。

---

- 如何构造区域 $R_1,R_2,\dots,R_J$
    
    我们将预测空间划分为高维的方框（*boxes*）以便简化和解释预测模型。而目标是找到的方框通过（8.1）可以最小化 $RSS$
    
    $$
    \sum\limits_{j=1}^{J}\sum\limits_{i\in R_j}(y_i-\hat{y}_{R_j})^2\tag{8.1}
    $$
    
    $\hat{y}_{R_j}$ 第 $j$ 个框内的训练观测值响应的平均。
    
    将特征值的每种划分都考虑到是不可行的。为此，我们采取一种自上而下的贪婪方法，即递归二进制分裂。
    
    该方法是自顶向下的，因为它从树(在这一点上所有的观测都属于一个区域)的顶端开始，然后依次分割预测变量空间，每个分裂通过树上向下延伸的两个新分支表示。它是贪婪的，因为在建树过程的每个步骤中，最好的分裂都是在那个特定的时候，而不是在未来的某个步骤中选择一个会导致更好的树的分裂。
    
    为了执行递归二进制分裂，我们首先选择预测变量 $X_j$ 和截断点 $s$ 使得将预测空间分裂成区域  $\{ X | X_j < s \}$  和 $\{ X | X_j≥s \}$ 时，会导致 $RSS$ 最大可能的减少。
    
    更详细地，对于任意的 $j$ 和 $s$，我们定义了半平面
    
    $$
    \begin{matrix}R_1(j,s)=\{X|X_j<s\}&\text{and}&R_2(j,s)=\{X|X_j\ge s\}\end{matrix}\tag{8.2}
    $$
    
    我们寻求使（8.3）最小的 $j$ 和 $s$ 的值
    
    $$
    \sum\limits_{i:x_i\in R_1(j,s)}(y_i-\hat y_{R_1})^2+\sum\limits_{i:x_i\in R_2(j,s)}(y_i-\hat y_{R_2})^2\tag{8.3}
    $$
    
    $\hat y_{R_i}$ 为 $R_i( j , s )$ 中训练观测的平均响应。
    
    接下来，我们重复这个过程，寻找最佳的预测器（$R_J$）和最佳的切点（s），以便进一步分割数据，从而最小化每个结果区域内的 $RSS$ 。然而，这一次，我们并没有对整个预测空间进行拆分，而是对之前识别的两个区域之一进行拆分。我们现在有三个区域。再次，我们希望进一步分割这三个区域中的一个，从而最小化 $RSS$ 。这一过程一直持续到达到停止标准。该算法示例如图8.3
    
    ![图8.3：左上角：二维特征空间的一个划分，不能由递归二进制拆分得到。右上角：二维例子上递归二进制分裂的输出。左下角：右上角面板中与分区对应的树。右下角：与该树对应的预测曲面的透视图。](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b/Untitled%201.png)
    
    图8.3：左上角：二维特征空间的一个划分，不能由递归二进制拆分得到。右上角：二维例子上递归二进制分裂的输出。左下角：右上角面板中与分区对应的树。右下角：与该树对应的预测曲面的透视图。
    

---

### 剪枝

上述过程可能会在训练集上产生良好的预测，但很可能会过拟合数据，导致测试集上性能不佳。这是因为生成的树可能过于复杂。较小的树和较少的分裂数(即区域 $R_1,\dots,R_J$  较少)可能会导致较低的方差和更好的解释，但代价是有一些偏差。对上述过程的一种可能的替代方法是，只要每次分裂导致的 $RSS$ 减少超过某个(高)阈值，就建立树。这种策略会导致较小的树，但这种方法过于短视，因为在树的早期一个看似毫无价值的分裂可能会是一个非常好的分裂——也就是说，一个分裂会导致以后的 $RSS$  的大量减少。

因此，一个较好的策略是生长一棵非常大的树 $T_0$，然后将其修剪回来，以获得一棵子树。我们如何确定最佳的方式来剪枝?

我们可以选择交叉验证或验证集来测试每一棵子树，但这样做代价过大。

代价复杂度剪枝（*Cost complexity pruning*）——也称为最弱连接剪枝（*weakest link pruning*）——给了我们一个方法来做到这一点。我们不考虑每一个可能的子树，而是考虑一个由非负调节参数 $\alpha$ 索引的树序列。对于每个 $\alpha$ 值，都有对应的子树 $T∈T0$ ，使得（8.4）尽可能小

$$
\sum\limits_{m=1}^{|T|}\sum\limits_{i:x_i\in R_m}(y_i-\hat{y}_{R_m})^2+\alpha|T|\tag{8.4}
$$

其中 $| T |$ 表示树 $T$ 的终端节点数，$R_m$ 是第 $m$ 个终端节点对应的矩形(即预测空间的子集)，$\hat y_{R_m}$ 是与 $R_m$  相关的预测响应，即 $R_m$  中训练观测值的均值。调节参数 $\alpha$  控制子树的复杂度与训练数据的拟合之间的权衡。当 $\alpha= 0$ 时，则子树 $T$ 将简单地等于 $T_0$ ，因为此时式( 8.4 )只是度量了训练误差。然而，随着 $\alpha$  的增加，拥有一棵许多终端节点的树是需要付出代价的，因此对于较小的子树，( 8.4 )将趋于最小化。式( 8.4 )令人想起 lasso ( 6.7 )，为了控制线性模型的复杂性，使用了类似的形式。

加入剪枝过程的回归树构建总结在算法8.1

---

算法8.1 构建回归树

1. 使用递归二进制分裂在训练数据上生长一棵大树，只有当每个终端节点的观测数少于某个最小值时停止。
2. 将代价复杂度修剪应用到大树中，以获得一系列最佳子树，作为的 $\alpha$ 的函数
3. 使用 $K$折交叉验证进行选择。即把训练观测值分成 $K$ 个折。对于每个 $k = 1，\dots,K$：
    1. 对除第 $k$ 折外的所有训练数据重复步骤1和步骤2。
    2. 评估第 $k$ 折中数据的均方预测误差，作为 $\alpha$ 的函数
    
    对每个值的结果取平均值，并截取使平均误差最小的值
    
4. 从步骤 2 返回与所选值 $\alpha$ 对应的子树

---

### 分类树

> 分类树与回归树非常相似，只是分类树用于预测定性响应而不是定量响应。对于一棵回归树，一个观测的预测响应是由属于同一终端节点（方框）的训练观测响应的平均给出。相反，对于一个分类树，我们预测每个观测属于其所属区域中最常出现的一类训练观测。在解释分类树的结果时，我们往往不仅对特定终端节点区域对应的类别预测感兴趣，而且对落入该区域的训练观测值中的类别**比例**感兴趣。
> 

分类树的任务与回归树的任务十分相似。正如在回归设置中，我们使用递归二进制分裂来生长一个分类树。然而，在分类设置中，$RSS$ 不能作为进行二进制拆分的标准。$RSS$ 的一个自然替代是**分类错误率（*classication error rate*）**。我们将给定区域中的一个观测分配给该区域中最常见的一类训练观测，因此分类错误率只是该区域中不属于最常见类别的训练观测的分数：

$$
E=1-\max\limits_k(\hat p_{mk})\tag{8.5}
$$

这里 $\hat p_{mk}$ 表示第 $m$ 个区域中来自第 $k$ 类的训练观测值的比例。然而，事实证明，分类误差对树生长并不十分敏感，在实际应用中，其他两种措施是可取的。

基尼系数（*Gini index*）

$$
G=\sum\limits_{k=1}^K\hat p_{mk}(1-\hat p_{mk})\tag{8.6}
$$

$K$ 类总方差的度量。不难看出，如果所有 $\hat p_{mk}$ 的都接近于0 或 1，则基尼系数取小值。因此，基尼指数被称为衡量节点纯度（*purity*）的指标——较小的值表明一个节点主要包含来自单个类的观测。

另一个指标是 熵（*entropy*）

$$
D=-\sum\limits_{k=1}^K\hat p_{mk}\log\hat p_{mk}\tag{8.7}
$$

由于$0≤\hat p_{mk}≤1$，所以 $0≤-\hat p_{mk} log\hat p_{mk}$ .我们可以证明，如果 $\hat p_{mk}$ 都接近于 0 或者接近于 1 ，那么熵将在零附近取值。因此，与基尼指数一样，如果第 $m$ 个节点是纯净的，那么熵也会呈现出一个较小的值。事实上，基尼系数和熵值在数值上非常接近。

在构建分类树时，通常使用基尼系数或熵值来评估特定分裂的质量，因为这两种方法对节点纯度的敏感度高于分类错误率。这 3 种方法中的任何一种都可以用于剪枝树，但如果以最终剪枝树的预测精度为目标，则分类错误率较好。

在我们到目前为止的讨论中，我们假定预测变量具有连续值。然而，即使存在定性的预测变量，也可以构建决策树。对定性变量进行拆分，相当于将部分定性值分配给一个分支，将剩余的定性值分配给另一个分支。

![图8.6：心脏数据集。顶部：未修剪的分类树。左下角：对于不同大小的剪枝树，交叉验证误差，训练和测试误差。右下角：最小交叉验证误差对应的剪枝树。](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b/Untitled%202.png)

图8.6：心脏数据集。顶部：未修剪的分类树。左下角：对于不同大小的剪枝树，交叉验证误差，训练和测试误差。右下角：最小交叉验证误差对应的剪枝树。

图8.6有一个令人惊讶的特点：一些分割产生了两个具有相同预测值的终端节点。例如，考虑未修剪树的右下角附近的拆分RestECG < 1。不管RestECG的值如何，对于那些观察值，预测的响应值是Yes。为什么？执行拆分是因为它会导致节点纯度增加。即右手叶对应的所有9个观测值的响应值为Yes，而左手叶对应的7/11个观测值的响应值为Yes。假设我们有一个试验观测，属于右手叶片给出的区域。那么我们可以很确定它的响应值是Yes。相比之下，如果一个测试观测属于左手叶片给出的区域，那么它的响应值很可能是Yes，但我们不太确定。虽然**分割后的RestECG < 1并没有降低分类误差，但提高了对节点纯度更敏感的Gini指数和熵。**

---

### 树与线性模型

线性回归假设模型形式

$$
f(X)=\beta_0+\sum_{j=1}^p X_j\beta_j\tag{8.8}
$$

回归树假设模型形式

$$
f(X)=\sum\limits_{m=1}^M c_m\cdot1_{(X\in R_m)}\tag{8.9}
$$

哪种模型更优？这取决于眼前的问题。如果特征和响应之间的关系像（ 8.8 ）中那样被线性模型很好地近似，那么线性回归方法可能会很好地工作，并且会优于不利用这种线性结构的回归树方法。如果如模型( 8.9 )所示，特征和响应之间存在高度非线性和复杂的关系，那么决策树可能优于经典方法。

---

### 树的优缺点

🙂

- 树非常容易解释。事实上，它们甚至比线性回归更容易解释
- 有人认为决策树比前面章节中的回归和分类方法更接近人类决策
- 树可以图形化显示，也很容易解读
- 树可以很容易地处理定性预测变量，而不需要创建虚拟变量

☹️

- 遗憾的是，树通常不具有与本书中其他一些回归和分类方法相同的预测精度
- 此外，树可能非常不健壮。换句话说，数据的一个小的变化可以引起最终估计树的一个大的变化

---

# Bagging、随机森林、Boosting和贝叶斯可加回归树

> 集成（*ensemb*）方法是一种将许多简单的“积木”模型组合在一起，以获得一个单一的和潜在的非常强大的模型的方法。这些简单的积木模型有时被称为弱学习器，因为它们本身可能导致平庸的预测
> 

### Bagging

第8.1节讨论的决策树有着高方差。这意味着，如果我们将训练数据随机分成两部分，并将决策树分成两半，那么我们得到的结果可能会截然不同。相比之下，方差较小的过程如果重复应用于不同的数据集，会得到相似的结果；当 $n$ 与 $p$ 的比值适中时，线性回归往往具有较低的方差。Bootstrap聚合或Bagging是一种减少统计学习方法方差的通用过程；它在决策树的上下文中特别有用和频繁使用

回想一下，给定一组 $n$ 个独立观测值 $Z_1,\dots,Z_n$，每个方差为 $\sigma^2$，观测值的均值 $\bar Z$ 的方差由$\sigma^2 / n$。给出.换句话说，平均一组观测值可以减小方差。因此，减少统计学习方法的方差和提高测试集精度的一个自然方法是从总体中抽取许多训练集，使用每个训练集建立一个单独的预测模型，并对结果进行平均。我们可以计算 $\hat f^1 ( x ),\hat f^2 ( x ),\dots,\hat f ^B ( x )$ 使用 $B$ 个单独的训练集，并对它们进行平均以获得单个低方差统计学习模型

$$
\hat{f}_\text{avg}(x)=\dfrac{1}{B}\sum\limits_{b=1}^B\hat{f}^b(x)
$$

当然，这是不实际的，因为我们一般无法获得多个训练集。相反，我们可以通过从(单个)训练数据集中抽取重复样本进行bootstrap。在该方法中，我们生成了 $B$ 个不同的 bootstrapped 训练数据集。然后在第 $b$ 个 bootstrapped 训练集上训练我们的方法，得到 $\hat f^{*b} ( x )$，并对所有预测进行平均得到

$$
\hat{f}_\text{bag}(x)=\dfrac{1}{B}\sum\limits_{b=1}^B\hat{f}^{*b}(x)
$$

到目前为止，我们已经描述了回归背景下的Bagging过程，以预测定量结果 $Y$。如何将 Bagging 推广到 $Y$ 为定性的分类问题？在这种情况下，但最简单的方法如下，对于给定的测试观测，我们可以记录它在每棵 $B$ 树预测的类，并采取多数票：总体预测是 $B$ 个预测中占比最多的类。

![图8.8： Bagging和随机森林结果用于Heart数据。试验误差(黑色和橙色)表示为使用的bootstrapped训练集个数 $B$ 的函数。随机森林应用 $m =\sqrt p$。虚线表示由单个分类树产生的测试错误。绿色和蓝色的轨迹显示OOB误差，在这种情况下相当低。](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b/Untitled%203.png)

图8.8： Bagging和随机森林结果用于Heart数据。试验误差(黑色和橙色)表示为使用的bootstrapped训练集个数 $B$ 的函数。随机森林应用 $m =\sqrt p$。虚线表示由单个分类树产生的测试错误。绿色和蓝色的轨迹显示OOB误差，在这种情况下相当低。

通过图8.8，bagging 测试错误率略低于单棵树的测试错误率。树木数量 $B$ 不是bagging 的关键参数，使用非常大的 $B$ 值不会导致过拟合。在实际中我们使用一个足够大的 $B$ 值，使得误差平息

---

### 袋外误差估计（*Out-of-Bag Error Estimation*)

事实证明，有一种非常直接的方法来估计bagging模型的测试误差，而不需要进行交叉验证或验证集方法。bagging 的关键是树是重复拟合到观测的bootstrapped 子集。可以表明，平均而言，每棵bagging树利用了大约三分之二的观测值。剩下的三分之一未被用于给定bagging树的观测值被称为袋外估计( *out-of-bag*，OOB )观测值。

我们可以利用观测为OOB的每棵树来预测第 $i$ 个观测的响应。对于第 $i$ 个观测，这将产生大约 $B/3$ 个预测。为了获得对第 $i$ 个观测的单个预测，我们可以对这些预测响应(如果回归是目标)进行平均，或者可以对(如果分类是目标)进行多数投票。这得到对第i个观测的单个 OOB 预测。对于 $n$ 个观测值中的每一个观测值，都可以通过这种方式得到一个 OOB 预测，从而计算出总体的 OOB $MSE$  （对于一个回归问题）或分类误差（对于一个分类问题）。由此产生的 OOB 误差是对袋装模型测试误差的有效估计，因为每个观测的响应只使用未使用该观测的树进行预测。[图8.8](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b.md) 显示了Heart数据上的 OOB 误差。可以看出，在 $B$ 充分大的情况下，OOB 误差几乎等价于留一交叉验证误差。在大数据集上执行Bagging时，用于估计测试误差的OOB方法特别方便，因为交叉验证需要大量的计算

---

### 重要变量度量

bagging 通常会导致使用单个树的预测精度提高。然而，不幸的是，解释由此产生的模型是困难的。回想一下，决策树的优点之一是结果的吸引力和易于解释的图。然而，当我们对大量的树进行打包时，不再可能用单棵树来表示由此产生的统计学习过程，也不再清楚哪些变量对该过程最重要。因此，Bagging以牺牲可解释性为代价来提高预测精度。

虽然 bagging 树比单个树的解释困难得多，但使用 $RSS$  （ 对于bagging回归树）或 Gini指数（对于bagging分类树）可以获得每个预测因子重要性的整体摘要。在Bagging回归树的情况下，我们可以记录 $RSS$  [( 8.1 )](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b.md)在给定的预测器上因分裂而减少的总量，在所有 $B$ 树上平均。一个大的值表明一个重要的预测因子。同理，在bagging 分类树的情况下，我们可以将基尼系数[( 8.6 )](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b.md)在给定的预测器上被拆分的总量加总，在所有 $B$ 树上取平均值。

---

### 随机森林

随机森林通过对树进行小的调整来提供对bagging 树的改进。与Bagging一样，我们在bootstrapped 训练样本上构建了多个决策树。但在构建这些决策树时，**每次考虑树中的分裂，从 $p$ 个预测变量的全集中选择 $m$ 个预测变量的随机样本作为分裂候选。分裂只允许使用这 $m$ 个预测因子中的一个。在每次分裂时取 $m$   个预测因子的新样本，通常我们选择 $m≈\sqrt p$**  ——即每次分裂时考虑的预测因子个数近似等于预测因子总数的平方根。

换句话说，在构建随机森林时，在树的每一个分裂处，算法不考虑大多数可用的预测变量。这可能听起来很疯狂，但它有一个聪明的理由。**假设数据集中有一个非常强的预测器**，以及其他一些中等强度的预测器。然后，在bagging 树的集合中，大多数或所有的树都将在顶部的拆分中使用这个强预测器。因此，所有的bagging 树看起来都非常相似，导致来自bagging 树的预测将是高度相关的。遗憾的是，对许多高度相关的量进行平均并不能带来像对许多不相关的量进行平均那样大的方差减少。这意味着**在此设置下，bagging 方法不会导致单棵树方差的大幅减小**。

随机森林通过迫使每个分裂只考虑预测变量的一个子集来克服这个问题。因此，平均而言 $( p-m) /p$ 的分裂甚至不会考虑强预报因子，所以其他预报因子会有更多的机会。我们可以把**这个过程看作是对树进行去相关**，从而使得生成的树的平均值的变化更小，因此更可靠。

**Bagging与随机森林的主要区别在于预测器子集大小 $m$ 的选择**。例如，如果使用 $m = p$ 构建随机森林，那么这相当于简单地bagging。在Heart数据上，使用$m =\sqrt p$ 的随机森林比 Bagging 减少了测试误差和OOB误差[（ 图8.8 ）](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b.md)。

---

### boosting

> 与Bagging一样，**Boosting是一种通用的方法，可以应用于许多用于回归或分类的统计学习方法。**
> 

回想一下，Bagging涉及到使用bootstrap创建原始训练数据集的多个副本，为每个副本建立单独的决策树，然后组合所有的树以创建单个预测模型。值得注意的是，每棵树都建立在一个bootstrap数据集上，独立于其他树。**Boosting的工作方式与此类似，只是树是按顺序生长的：每棵树都是利用先前生长的树的信息生长的。Boosting不涉及bootstrap抽样；相反，每棵树都建立于原始数据集的修改版本上。**

---

算法8.2 回归树的Boosting

1. 设定 $\hat f(x)=0$ 且 训练集中所有的 $i$ 有$r_i=y_i$
2. 对于 $b=1,2,\dots,B$ 
    1. 将具有 $d$ 个分裂（$d + 1$ 个终端节点）的树 $\hat f^b$ 拟合到训练数据 $(X,r)$
    2. t通过添加新树的收缩版来更新 $\hat f$ 
        
        $$
        \hat{f}(x)\leftarrow\hat{f}(x)+\lambda\hat{f}^b(x)\tag{8.10}
        $$
        
    3. 更新残差
        
        $$
        r_i\leftarrow r_i-\lambda\hat{f^b}(x_i)\tag{8.11}
        $$
        
3. 输出模型
    
    $$
    \hat{f}(x)=\sum\limits_{b=1}^B\lambda\hat{f}^b(x)\tag{8.12}
    $$
    

---

不同于对数据进行单个大决策树的处理，这相当于对数据进行硬处理和潜在的过度处理，boosting 方法反而学习缓慢。给定当前模型，我们**对模型的残差进行决策树。也就是说，我们使用当前残差而不是结果$Y$作为响应的树。**然后我们将这个新的决策树添加到拟合函数中，以更新残差。这些树中的每一个都可以很小，只有几个终端节点，由算法中的参数 $d$ 决定。通过将小树拟合到残差中，我们在表现不佳的区域慢慢提高  $\hat f$ 。收缩参数进一步减缓了过程，允许更多不同形状的树加入残差。一般来说，学习缓慢的统计学习方法往往表现良好。注意，在提升中，与bagging不同的是，**每棵树的构建对已经生成的树都有很强的依赖性**

Boosting有三个调优参数：

- 与Bagging和随机森林不同的是，如果 $B$ 太大，则 boosting 会过拟合，尽管这种过拟合倾向于缓慢发生。我们使用交叉验证来选择 $B$
- 收缩参数 $\lambda$，一个很小的正数。这控制了提升学习的速率。典型值为0.01 或 0.001，正确的选择可以视问题而定。非常小的 $\lambda$ 可以要求使用非常大的 $B$ 值以达到良好的性能
- 每棵树的分裂数 $d$ 控制了boosting集成的复杂度。通常 $d = 1$ 效果很好，在这种情况下，每棵树都是一个树桩（*stump*），由单个分裂组成。在这种情况下，boosting 的集合是一个可加模型，因为每个项只涉及单个变量。更一般的 $d$ 是交互深度，控制了 boosting 模型的交互顺序，因此 $d$ 个拆分最多可以涉及 $d$  个变量。

![图8.11](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b/Untitled%204.png)

图8.11

在图8.11中，我们将Boosting应用于15类癌症基因表达数据集，以开发一个可以区分正常类和14个癌症类的分类器。我们将测试误差表示为树的总数和交互深度 $d$ 的函数。我们看到，如果包含足够多的相互作用深度为 1 的简单树桩，它们的表现就很好，该模型优于深度二模型，且均优于随机森林。**这突出了boosting森林与随机森林的一个区别：在boosting中，由于某棵树的生长考虑到了已经生长的其他树，所以较小的树通常是充足的。使用较小的树也有助于可解释性，例如，使用树桩会得到一个加性模型**

---

### 贝叶斯可加回归树（*Bayesian Additive Regression Trees，*BART）

在介绍BART算法之前，我们先定义了一些符号。令 $K$ 表示回归树的数量，$B$ 表示 BART 算法运行的迭代次数。$\hat f_k^b(x)$ 表示第 $b$ 次迭代中使用的第 $k$ 个回归树在 $x$ 处的预测。每次迭代结束后，将该次迭代得到的 $K$ 棵树进行求和，例如，$\begin{aligned}\hat{f}^b(x)=\sum_{k=1}^K\hat{f}^b_k(x)\stackrel{}{，}b=1,\ldots,B\end{aligned}$

在BART算法的第 $r$ 次迭代中，所有树被初始化为只有一个根节点，$\hat{f}_k^1(x)=\frac{1}{nK}\sum_{i=1}^n y_i$，为响应值的均值除以树的总数。因此，$\begin{aligned}\hat{f}^1(x)=\sum_{k=1}^K\hat{f}^1_k(x)=\frac{1}{n}\sum_{i=1}^n y_i\quad\text{}\end{aligned}$

在后续的迭代中，BART更新这 $K$ 棵树，每次更新一棵。在第 $b$ 次迭代中，为了更新第 $k$ 棵树，我们从每个响应值中减去除第 $k$ 棵树以外的所有预测值，以获得第 $i$ 个观测值部分残差

$$
r_i=y_i-\sum\limits_{k'<k}\hat{f}_{k'}^b(x_i)-\sum\limits_{k'>k}\hat{f}_{k'}^{b-1}(x_i)
$$

BART 不是将新树拟合到该部分残差，而是从一组可能的扰动（*perturbation*）中随机选择前一次 （$\hat{f}_{k}^{b-1}$）迭代对树的扰动，偏向于提高对部分残差的拟合度的扰动。 这种扰动有两个组成部分：

1. 我们可以通过添加或剪枝来改变树的结构。
2. 我们可以改变树的每个终端节点的预测。

![图8.12：BART算法的扰动树示意图。( a )：显示第 $( b-1 )$ 次迭代时的第 $k$ 棵树$\hat f ^{b-1}_k ( X )$。面板( b ) - ( d )展示了给定 $\hat f^{b-1}_k ( X )$ 的形式，$\hat f^b_k ( X )$ 的三种可能性。( b )：一种可能是$\hat f^b_ k ( X )$ 与$\hat f^{b -1}_ k ( X )$ 具有相同的结构，但在终端节点处有不同的预测。( c )：另一种可能是$\hat f^b_ k ( X )$ 是剪枝 $\hat f^{b-1}_ k ( X )$ 的结果。( d )：或者，$\hat f^b_ k ( X )$ 可能比 $\hat f^{b -1}_ k ( X )$ 有更多的终端节点.](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b/Untitled%205.png)

图8.12：BART算法的扰动树示意图。( a )：显示第 $( b-1 )$ 次迭代时的第 $k$ 棵树$\hat f ^{b-1}_k ( X )$。面板( b ) - ( d )展示了给定 $\hat f^{b-1}_k ( X )$ 的形式，$\hat f^b_k ( X )$ 的三种可能性。( b )：一种可能是$\hat f^b_ k ( X )$ 与$\hat f^{b -1}_ k ( X )$ 具有相同的结构，但在终端节点处有不同的预测。( c )：另一种可能是$\hat f^b_ k ( X )$ 是剪枝 $\hat f^{b-1}_ k ( X )$ 的结果。( d )：或者，$\hat f^b_ k ( X )$ 可能比 $\hat f^{b -1}_ k ( X )$ 有更多的终端节点.

我们通常舍弃这些预测模型中的前几个，因为在较早的迭代中得到的模型——称为预模拟（*burn-in*）运行期——往往不能提供很好的结果。令 $L$ 表示预模拟运行迭代次数。例如，可以取 $L = 200$。然后，为了获得单次预测，我们简单地取预模拟运行次迭代后的平均值，$\hat{f}(x)=\frac{1}{B-L}\sum_{b=L+1}^{B}\hat{f}^{b}(x)$ 然而，也可以计算除平均数以外的量。整个BART过程总结在算法8.3中。

---

算法8.3 贝叶斯可加回归树

1. 令 $\hat{f}_1^1(x)=\hat{f}_2^1(x)=\cdots=\hat{f}_K^1(x)=\frac{1}{nK}\sum_{i=1}^n y_i$
2. 计算 $\hat{f}^{1}(x)=\sum_{k=1}^{K}\hat{f}^{1}_{k}(x)=\frac{1}{n}\sum_{i=1}^{n}y_{i}$
3. 对于 $b=2,\dots,B$
    1. 对于 $k=1,2,\dots,K$
        1. 对于 $i=1,\dots,n$ ，计算当前部分残差
            
            $$
            r_i=y_i-\sum_{k'<k}\hat{f}_{k'}^{3}(x_i)-\sum_{k'>k}\hat{f}_{k'}^{3-1}(x_i)
            $$
            
        2. 通过随机扰动上一次迭代中的第 $k$ 棵树$\hat f^{b-1}_ k ( x )$，拟合一棵新树 $\hat f^b_ k ( x )$ 。提高拟合的扰动是有利的
    2. 计算$\hat f^b(x)=\sum_{k=1}^K\hat f^b_k(x)$
4. 计算 $L$ 个预模拟后的均值
    
    $$
    \hat{f}(x)=\dfrac{1}{B-L}\sum\limits_{b=L+1}^B\hat{f}^b(x)
    $$
    

---

BART方法的一个关键要素是，在步骤3 ( a ) ii 中，我们并没有对当前部分残差生成新的树，而是通过对上一次迭代中得到的树(见图8.12)略作修改，尝试将拟合改进为当前部分残差。粗略地说，这可以防止过拟合，因为它限制了每次迭代中数据的难易程度。此外，个体树通常非常小。我们限制树的大小是为了避免过拟合数据，如果我们生长非常大的树，过拟合更有可能发生。

图8.13给出了当迭代次数增加到10000时，使用 $K = 200$ 棵树对数据应用BART的结果。在初始迭代过程中，测试误差和训练误差在一点点附近跳跃。经过这个初始的预模拟期后，错误率趋于稳定。我们注意到训练误差和测试误差之间只有很小的差异，说明树扰动过程很大程度上避免了过拟合

![图8.13：经过100次迭代(以灰色显示)后，BART的错误率基本稳定。Boosting在几百次迭代后开始过拟合。](Tree-Based%20Methods%2081a9d36d64b144ccb689762a2769a28b/Untitled%206.png)

图8.13：经过100次迭代(以灰色显示)后，BART的错误率基本稳定。Boosting在几百次迭代后开始过拟合。

BART方法可以被看作是对一棵树集合进行拟合的一种贝叶斯方法：每次我们随机扰动一棵树获得残差，实际上是在从后验分布中绘制一棵新树。此外，算法8.3可以看作是拟合BART模型的马尔科夫链蒙特卡罗算法。

当我们应用BART时，我们必须选择树的数量 $K$ ，迭代次数 $B$ 和预模拟迭代次数 $L$。我们通常为 $B$ 和 $K$ 选择较大的值，为 $L$ 选择适中的值，例如$K = 200$，$B = 1000$，$L = 100$ 是合理的选择。BART已经被证明具有非常令人印象深刻的开箱即用性能——即在最小的调优下表现良好。

---

### 树的集成方法综述（*Summary of Tree Ensemble Methods*）

树是集成方法中弱学习器的一个有吸引力的选择，原因有很多，包括它们的灵活性和处理混合类型(即定性与定量相结合)的预测的能力。我们现在已经看到了4种对树集合进行拟合的方法：Bagging、随机森林、Boosting和BART。

- 在随机森林中，树独立地生长在观测的随机样本上。然而，每棵树上的每一次分裂都是使用特征的一个随机子集进行的，从而去除了树的相关性，并导致相对于Bagging更全面的模型空间探索
- 在提boosting中，我们只使用原始数据，不抽取任何随机样本。树是依次生长的，使用一种缓慢的学习方法：每棵新树都是拟合到从较早的树上遗留下来的信号，并且在使用之前收缩（*shrunken*）。
- 在BART中，我们再次只利用了原始数据，并依次进行了树生长。然而，为了避免局部极小值，实现对模型空间更全面的探索，对每棵树进行扰动。