# Moving Beyond Linearity

> 线性模型相比其他模型有很好的解释性和推断，但在其只能是近似数据，所以在预测能力上较弱。在这一章中，我们放宽了线性假设，但仍然试图保持尽可能多的可解释性。我们通过考察多项式回归和阶跃函数等线性模型的非常简单的扩展，以及曲线函数（splines）、局部回归和广义可加模型等更复杂的方法来做到这一点。
> 

- **多项式回归**通过增加额外的预测变量来扩展线性模型，通过将每个原始预测变量提高到一个幂次来获得。例如，三次回归使用 $X$、$X^2$ 和 $X^3$ 三个变量作为预测变量。该方法提供了一种向数据提供非线性拟合的简单方法。
- **阶跃函数**将变量的值域划分为 K 个不同的区域，以产生一个定性变量。这具有拟合分段常数函数的作用。
- **回归曲线**比多项式和阶跃函数更灵活，实际上是两者的扩展。它们涉及将 $X$ 的范围划分为 K 个不同的区域。在每个区域内，对数据用一个多项式函数拟合。然而，**这些多项式是受约束的，以便它们在区域边界或节点处顺利连接**。如果区间被划分为足够多的区域，这会产生一个极其灵活的拟合。
- **平滑曲线**与回归曲线类似，但出现的情况略有不同。平滑曲线的结果是最小化一个带有平滑性惩罚的残差平方和准则。
- **局部回归**类似于回归曲线，但有一个重要的区别。区域被允许重叠，而且它们确实以非常平滑的方式这样做
- **广义可加模型**允许我们扩展上述方法来处理多个预测变量。

---

# 多项式回归（**********polynomial regression**********)

一般来说，将线性回归推广到预测变量与响应变量之间的关系是非线性的情况的标准方法是替换标准的线性模型

$$
y_i=\beta_0+\beta_1x_i+\epsilon_i
$$

用多项式函数表示为

$$
y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\cdots+\beta_dx_i^d+\epsilon_i\tag{7.1}
$$

一般来说，使用 $d$ 大于 3 或 4 是不常见的，因为对于较大的 $d$ 值，多项式曲线会变得过于灵活，并且会呈现出一些非常奇怪的形状

![左边：实蓝色曲线是工资(单位：千美元)关于年龄的 4 次多项式。虚线表示估计的 95% 置信区间。右边：我们使用逻辑回归对二元事件（工资 > 250）进行建模，同样使用 4 次多项式。工资超过 25 万美元的后验概率以蓝色显示，连同估计的 95% 置信区间。](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled.png)

左边：实蓝色曲线是工资(单位：千美元)关于年龄的 4 次多项式。虚线表示估计的 95% 置信区间。右边：我们使用逻辑回归对二元事件（工资 > 250）进行建模，同样使用 4 次多项式。工资超过 25 万美元的后验概率以蓝色显示，连同估计的 95% 置信区间。

在图中，一对虚线曲线伴随着拟合实现；这些是两倍的标准误曲线。我们来看看这些是如何产生的。假设我们计算了某一特定年龄 $x_o$ 下的拟合

$$
\hat{f}(x_0)=\hat{\beta}_0+\hat{\beta}_1x_0+\hat{\beta}_2x_0^2+\hat{\beta}_3x_0^3+\hat{\beta}_4x_0^4\tag{7.2}
$$

该拟合的方差为多少？最小二乘可以返回每个系数 $\hat\beta_ j$ 的方差估计值，以及系数估计对之间的协方差。可以用这些来计算方差（如果 $\hat C$  是 $\hat\beta_j$ 的 5 × 5 协方差矩，$\ell_0^T=(1,x_0,x_0^2,x_0^3,x_0^4)$ ，则 $\mathrm{Var}[\hat{f}(x_{0})]=\ell_{0}^{T}\hat{\mathbf{C}}\ell_{0}$ ）估计的 $\hat f ( x_0 )$ 的逐点标准误差是这个方差的平方根。在每个参考点 $x_0$ 处重复该计算，绘制拟合曲线，并在拟合曲线的任一侧绘制 2 倍的标准误差，2 倍因为对于正态分布的误差项，这个量对应一个大约 95% 的置信区间

图中的工资似乎来自两个不同的人群：似乎有一个收入超过25万美元的高收入，以及低收入者群体。我们可以通过将工资分解为这两个组别，将其视为一个二元变量。然后可以使用 Logistic 回归来预测这种二元响应，使用年龄的多项式函数作为预测因子。拟合模型：

$$
\Pr(y_i>250|x_i)=\dfrac{\exp(\beta_0+\beta_1x_i+\beta_2x_i^2+\cdots+\beta_d x_i^d)}{1+\exp(\beta_0+\beta_1x_i+\beta_2x_i^2+\cdots+\beta_d x_i^d)}\tag{7.3}
$$

结果如图中右图所示。面板顶部和底部的灰色标记分别表示高收入者和低收入者的年龄。实蓝色曲线表示作为年龄的函数，成为高收入者的概率。同时给出了 95% 置信区间的估计值。我们看到这里的置信区间相当宽，尤其是在右边。尽管这个数据集的样本量很大，但只有79个高收入者，这导致估计系数的方差很高，从而导致宽的置信区间。

---

# 阶梯函数（**************step functions**************）

> 在线性模型中使用特征的多项式函数作为预测因子会对X的非线性函数施加一个全局结构。为了避免施加这样的全局结构，我们可以使用阶跃函数。
> 

这里我们将X的取值范围分解为若干区间，并在每个区间内取不同的常数。这相当于将一个连续变量转换为有序分类变量。更详细地，我们创建了截断点$c_1，c_2,\dots,c_K$ 在 $X$ 的范围内，然后构造 $K + 1$ 个新变量

$$
\begin{array}{rcl}C_0(X)&=&I(X<c_1),\\ C_1(X)&=&I(c_1\leq X<c_2),\\ C_2(X)&=&I(c_2\leq X<c_3),\\ &\vdots\\ C_{K-1}(X)&=&I(c_{K-1}\leq X<c_K),\\ C_K(X)&=&I(c_{K}\leq X),\end{array}\tag{7.4}
$$

$I ( · )$ 是一个指示函数，如果条件成立则返回 1，否则返回 0。例如，如果 $c_K≤X$，则 $I( cK≤X)$ 等于 1，否则等于 0。这些有时被称为虚拟变量。注意，对于 $X$ 的任意值，$C_0 ( X ) + C_1 ( X ) + · · · + C_K ( X ) = 1$，因为 $X$ 必须恰好位于 $K + 1$ 个区间中的一个。然后我们用最小二乘法建立线性模型，$C_1 ( X ),C_2 ( X ),\dots,C_k ( X )$ 作为预测因子:

$$
y_i=\beta_0+\beta_1C_1(x_i)+\beta_2C_2(x_i)+\cdots+\beta_KC_K(x_i)+\epsilon_i\tag{7.5}
$$

![与上节相同，使用最小二乘回归，不同的是拟合函数为阶梯函数](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled%201.png)

与上节相同，使用最小二乘回归，不同的是拟合函数为阶梯函数

遗憾的是，除非预测器中存在自然断点，否则分段常数函数可能会遗漏细节。例如，在上图的左边面板中，第一个统计堆（first bin）明显遗漏了工资随年龄增长的趋势。然而，在生物统计学和流行病学等学科中，阶跃函数方法非常流行。

---

# 基函数（*basis functions*）

> 多项式和分段常数回归模型实际上是基函数方法的特例。其思想是手里有一个可以应用于变量X的函数或变换的函数：$b_1 ( X ),b_2 ( X ),\dots,b_K ( X )$
> 

我们拟合以下模型来代替直接对 $X$ 拟合线性模型

$$
y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)+\beta_3b_3(x_i)+\cdots+\beta_K b_K(x_i)+\epsilon_i\tag{7.7}
$$

注意到基函数 $b_1 ( X ),b_2 ( X ),\dots,b_K ( X )$  是已知的。因此，我们可以用最小二乘法来估计( 7.7 )中的未知回归系数。重要的是，这意味着在第三章中讨论的线性模型的所有推断工具，例如系数估计值的标准误和模型整体显著性的 F-统计量，都可以在这个设定中使用。

---

# 回归曲线（*****************regression splines*****************）

### 分段式多项式（ piecewise polynomials ）

> 分段多项式回归不是在X的整个范围内拟合一个高次多项式，而是在X的不同区域上拟合单独的低次多项式。
> 

例如，三次分段多项式通过拟合一个三次回归模型来工作：

$$
y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\beta_3x_i^3+\epsilon_i\tag{7.8}
$$

系数 $\beta_0、\beta_1、\beta_2、\beta_3$ 在 $X$ 取值范围内的不同部分，分别不同。系数发生变化的点称为节点。

例如，无节点的分段三次多项式就是标准的三次多项式，如式 ( 7.1 )中d = 3。在点 $c$ 处具有单个结点的分段三次多项式具有如下形式：

$$
y_i=\begin{cases}\beta_{01}+\beta_{11}x_i+\beta_{21}x_i^2+\beta_{31}x_i^3+\epsilon_i&\text{if}\space \space x_i<c\\ \beta_{02}+\beta_{12}x_i+\beta_{22}x_i^2+\beta_{32}x_i^3+\epsilon_i&\text{if}\space\space x_i\geq c\end{cases}
$$

这种分段可能会带来一个问题，拟合函数可能不是连续的

![Untitled](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled%202.png)

---

### 约束曲线（ constraints and splines）

> 上一节的图看起来不对，因为拟合曲线太灵活了。为了解决这个问题，我们可以在拟合曲线必须连续的约束条件下，构造分段多项式
> 

也就是说，age = 50时不能出现跳跃。下图显示了调整后得到的拟合。这看起来比之前的情况要好，但是 V 型连接看起来不自然

![Untitled](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled%203.png)

在下图中，我们增加了两个额外的约束：现在分段多项式的一阶和二阶导数在age = 50 处都是连续导数。也就是说，我们要求分段多项式不仅在 age= 50 时连续，而且非常光滑。我们**对分段三次多项式施加的每一个约束都有效地释放了一个自由度，降低了所得分段多项式拟合的复杂度**。

![Untitled](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled%204.png)

所以在上一节图中，我们使用了八个自由度，但是在最后的图中我们施加了三个约束(连续性,一阶导数的连续性,二阶导数的连续性)，剩下了五个自由度。最后图中的曲线称为三次样条。一般来说，一个有 $K$ 个节点的三次样条使用 $4 + K$ 个自由度

---

### 曲线的基表示（ The Spline Basis Representation ）

> 我们在上一节中看到的回归样条可能看起来有些复杂：如何在分段次数为d的多项式连续的约束下进行回归? 事实证明，我们可以用基模型( 7.7 )来表示一个回归样条。
> 

具有K个节点的三次样条可以建模为

$$
y_i=\beta_0+\beta_1b_1(x_i)+\beta_2b_2(x_i)+\cdots+\beta_{K+3}b_{K+3}(x_i)+\epsilon_i\tag{7.9}
$$

对于基函数 $b_1,b_2,\dots,b_{K+3}$ 的适当选择，则模型( 7.9 )可以用最小二乘法拟合

---

### 选择节点的数量和位置

当我们做样条时，我们应该把结点放在哪里? 回归样条在包含大量节点的区域中最灵活，因为在这些区域中多项式系数可以快速变化。因此，一种选择是在我们觉得函数变化最快的地方放置更多的结，在更稳定的地方放置更少的结。虽然这个方法可以很好地工作，但在实践中，以统一的方式放置结是常见的。这样做的一种方法是，指定所需的自由度，然后让软件在数据的均匀分位数上自动放置相应数目的节点。

我们应该使用多少个节点，或者等价地，我们的样条应该包含多少个自由度? 一种选择是尝试不同数量的节点，看看哪个产生了最佳的外观曲线。更客观的方法是使用交叉验证，如第5章和第6章所述。通过这种方法，我们将一部分数据(如10 %)，用一定数量的节点的样条移到剩余的数据中拟合，然后使用样条对保留的数据进行预测。我们重复这个过程多次，然后计算整体的交叉验证RSS。对于不同的节点数 K，可以重复这个过程。然后选择使RSS最小的K值。

---

### 与多项式回归的比较

图7.7 在Wage数据集上比较了一个具有15个自由度的自然三次样条和一个15次多项式。多项式中的额外灵活度在边界处产生不希望的结果，而自然三次样条仍然为数据提供合理的拟合。回归样条往往能给出优于多项式回归的结果。这是因为与多项式必须使用高次(最高一次项中的指数,例如. $X^{15}$ )才能产生灵活的拟合，样条通过增加节点数但保持合理的维度来引入灵活度。一般来说，这种方法产生更稳定的估计。样条还允许我们在函数 $f$ 看起来变化很快的区域上放置更多的节点，从而具有灵活性，并且在 $f$ 看起来更稳定的区域上放置更少的节点。

![Untitled](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled%205.png)

---

# 平滑曲线（*****************smoothing splines*****************）

### 平滑曲线概述

寻找一个**平滑**函数 $g(x)$ 使得 $RSS$ 最小，一种方法是最小化

$$
\sum\limits_{i=1}^n(y_i-g(x_i))^2+\lambda\int g''(t)^2dt\tag{7.11}
$$

$\lambda$  是非负的调和参数。最小化（7.11）的函数 $g$ 称为平滑曲线

（7.11）公式是 “ 损失＋回归 ” 的形式。第一项是为了更好的拟合数据，第二项（用于控制曲线的平滑性）出现二阶导数是因为，如果函数在某个数 $t$ 附近是非平滑的，那么它的绝对值较大，否则接近于 0 。如果 $g$ 非常光滑，那么  $g'(t)$ 将接近常数，$\int g''(t)^2dt$ 将得到一个很小的值。因此，$\lambda\int g''(t)^2dt$ 鼓励 $g$ 是光滑的，$\lambda$ 越大，$g$ 越光滑

 当 $\lambda=0$ 时，( 7.11 )式中的惩罚项不起作用，因此函数 $g$ 将非常跳跃，将精确地插值训练观测值。当 $\lambda →∞$ 时，$g$ 将是完全光滑的——它将只是一条尽可能靠近训练点的直线。事实上，在这种情况下，$g$ 将是线性最小二乘直线，因为( 7.11 ) 中的损失函数等于最小化残差平方和。对于中间值，g会逼近训练观测值，但会有些平滑。我们看到，$\lambda$ 控制了平滑曲线的偏差-方差权衡。

### 平滑参数 $\lambda$ 的选择

> 调节参数 $\lambda$ 控制平滑样条的粗糙度，从而控制有效自由度。
> 

在光滑样条的背景下，我们为什么要讨论有效自由度而不是自由度?

虽然光滑样条有 $n$ 个参数，从而有 $n$ 个名义自由度，但这 $n$ 个参数受到了严重的约束或收缩。因此，$df_\lambda$ 是平滑样条可伸缩性的度量——它越高，平滑样条的灵活性(以及较低的偏差但较高的方差)越大。

有效灵活度写为

$$
\hat{\mathbf{g}}_\lambda=\mathbf{S}_\lambda\mathbf{y}\tag{7.12}
$$

$\hat{\mathbf{g}}_\lambda$ 是( 7.11 )对应一个特定 $\lambda$ 的解，即它是一个平滑曲线在训练点 $x_1,\dots,x_n$ 处拟合的 $n$ 维向量。式( 7.12 )表明，对数据应用平滑曲线时的拟合值向量可以写成 $n × n$ 矩阵 $\mathbf{S}_\lambda$ (对于其中有一个公式)乘以响应向量 $\mathbf y$。则有效自由度定义为

$$
df_\lambda=\sum\limits_{i=1}^n\{\mathbf{S}_\lambda\}_{ii}\tag{7.13}
$$

矩阵 $\mathbf{S}_\lambda$ 的对角元素之和。

我们需要选择合适的 $\lambda$ ，一种解决方法是交叉验证。留一法交叉验证误差（LOOCV）可以非常有效地计算，代价与计算单个拟合的代价基本相同：

$$
\operatorname{RSS}_{cv}(\lambda)=\sum_{i=1}^n(y_i-\hat{g}_\lambda^{(-i)}(x_i))^2=\sum_{i=1}^n\left[\dfrac{y_i-\hat{g}_\lambda(x_i)}{1-\{\mathbf{S}_\lambda\}_{ii}}\right]^2
$$

$\hat{g}_\lambda^{(-i)}(x_i))$ 为该平滑曲线在验证数据上的拟合值。选择使该值最小的 $\lambda$

![Untitled](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled%206.png)

对工资数据进行样条平滑处理。红色曲线是指定 16 个有效自由度的结果。对于蓝色曲线，通过留一法交叉验证自动发现，得到了 6.8 的有效自由度。两者区别不大，6.8 自由度是可取的。

---

# 局部回归（*local regression*）

> 局部回归是拟合灵活非线性函数的一种不同方法，仅利用附近的训练观测值计算目标点 $x_0$ 处的拟合
> 

![Untitled](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled%207.png)

局部回归在一些模拟数据上进行说明，其中蓝色曲线表示数据产生的 $f ( x )$，浅橙色曲线对应局部回归估计 $\hat f ( x )$ 。橙色点位于目标点 $x_0$ 的局部，用橙色垂线表示。叠加在图上的黄色钟形表示分配给每个点的权重，随着距离目标点的距离减小到零。在 $x_0$ 处的拟合 $\hat f ( x_0 )$ 由加权线性回归(橙色线段)拟合得到，并代入$x_0$ 拟合(橙色实心圆点)作为$\hat f ( x_0 )$ 的估计值。

---

局部回归算法：

1. 收集离 $x_0$ 最近的训练点 $x_i$ 的分数 $s=k/n$
2. 给领域内的每个点分配一个权重 $K_{i0}=K(x_i,x_0)$ 使得距离 $x_0$ 最远的点权重为零，距离 $x_0$ 最近的点权重最大。除了这 $k$  个近邻外，其他近邻的权重都为零。
3. 使用上述权重对 $x_i$ 上的 $y_i$ 进行加权最小二乘回归，通过寻找 $\hat\beta_i\space(i=0,1)$ 最小化公式：
    
    $$
    \sum\limits_{i=1}^{n}K_{i0}(y_i-\beta_0-\beta_1x_i)^2\tag{7.14}
    $$
    
4. 由公式 $\hat{f}(x_0)=\hat{\beta}_0+\hat{\beta}_1x_0$ 拟合 $x_0$ 处的值

---

对于 $x_0$ 的每一个值，权重 $K_{i0}$ 都会发生变化。也就是说，为了得到新点处的局部回归，需要对一组新的权重通过最小化( 7.14 )来得到新的加权最小二乘回归模型。

最重要的选择是跨度s，即在 $x_0$ 处用于计算局部回归的点的比例，如上面步骤1所确定的。跨度的作用类似于平滑样条中的调整参数 $\lambda$ ：它控制非线性拟合的灵活性。s 的值越小，我们的拟合就越局部和摇摆，或者，非常大的 s 值将导致使用所有训练观察值对数据进行全局拟合。我们可以再次使用交叉验证来选择 s，或直接指定。

![s = 0.7 比 s = 0.2 的曲线更平滑](Moving%20Beyond%20Linearity%20e28f2f7d23c64d5dad741ad1e5171128/Untitled%208.png)

s = 0.7 比 s = 0.2 的曲线更平滑

---

# 广义相加模型（*generalized additive models*）

> 广义可加模型( Generalized Additive Models，GAMs )允许每个变量为非线性函数，同时保持可加性，以此提供了一个通用框架来扩展标准线性模型。如同线性模型一样，GAMs可以同时应用于定量和定性反应。
> 

---

### 回归问题的GMAs

一种扩展多元线性回归的自然方法是

$$
y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}+\epsilon_i
$$

为了允许每个特征与响应之间存在非线性关系，用一个(光滑)非线性函数$f_j ( x_{ij })$代替每个线性分量 $\beta_jx_{ij}$ 。

$$
\begin{array}{rcl}y_i&=&\beta_0+\sum_{j=1}^p\int_j(x_{ij})+\epsilon_i\\ &=&\beta_0+f_1(x_{i1})+f_2(x_{i2})+\cdots+f_p(x_{ip})+\epsilon_i\end{array}\tag{7.15}
$$

之所以称之为加性模型，是因为我们为每个$X_j$ 计算一个单独的 $f_j$ ，然后将它们的所有贡献加在一起

---

### GAMs的优点与局限性

- 🙂 GAMs允许我们对每个$X_j$ 赋予一个非线性 $f_j$ ，这样我们就可以自动建立起标准线性回归会遗漏的非线性关系。这意味着我们不需要单独手动尝试每个变量的不同变换。
- 🙂 非线性拟合有可能对响应Y做出更准确的预测。
- 🙂 由于模型是可加的，我们可以在保留所有其他已拟合变量的情况下，单独考察每个$X_j$ 对 $Y$ 的影响。
- 🙂 函数 $f_j$ 对于变量 $X_j$ 的光滑性可以通过自由度来概括。
- ☹️ GAMs的主要局限性在于模型受限于可加性。由于变量较多，重要的交互作用可能被遗漏。然而，与线性回归一样，我们可以通过在GAM模型中加入额外的 $X_j × X_k$ 形式的预测因子来手动添加交互项。此外，我们可以在模型中加入 $f_{jk}( X_j ,X_k)$ 形式的低维相互作用函数，这类项可以使用二维平滑器，如局部回归，或者二维曲线(此处未涉及)。

---

### 分类问题的GAMs

GAMs也可以用于 $Y$ 是定性的情况。为了简单起见，这里我们假设 $Y$ 取 0 或 1 ，并令 $p ( X ) = Pr( Y = 1 | X)$ 为响应等于 1 的条件概率(给定预测因子)。回想逻辑回归模型( 4.6 )：

$$
\log\left(\dfrac{p(X)}{1-p(X)}\right)=\beta_0+\beta_1X_1+\beta_2X_2+\cdots+\beta_pX_p\tag{7.17}
$$

扩展（7.17）的非线性是通过：

$$
\log\left(\dfrac{p(X)}{1-p(X)}\right)=\beta_0+f_1(X_1)+f_2(X_2)+\cdots+f_p(X_p)\tag{7.18}
$$

（7.18）是 logist 回归的 GAM 形式与上一节中讨论的定量响应具有相同的优点和缺点。